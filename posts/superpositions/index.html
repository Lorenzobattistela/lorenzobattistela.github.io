<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1.0" data-next-head=""/><meta name="author" content="Lorenzo Battistela" data-next-head=""/><link rel="icon" href="/favicon.ico" data-next-head=""/><title data-next-head=""></title><meta name="description" content="Understanding HVM superpositions" data-next-head=""/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/966ef0bc2ff51cf6.css" as="style"/><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&amp;family=IBM+Plex+Sans:wght@400;500;600;700&amp;display=swap" rel="stylesheet"/><link rel="stylesheet" href="/_next/static/css/966ef0bc2ff51cf6.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-afbe389e5387c555.js" defer=""></script><script src="/_next/static/chunks/framework-d7f578ab3069408c.js" defer=""></script><script src="/_next/static/chunks/main-a5703b490a653431.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d35945f6f2ae940c.js" defer=""></script><script src="/_next/static/chunks/230-8a10a6030a242aaa.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-9da8c7586f67ce5f.js" defer=""></script><script src="/_next/static/ydYcoKj9y-mj7XR-0MT3a/_buildManifest.js" defer=""></script><script src="/_next/static/ydYcoKj9y-mj7XR-0MT3a/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="container"><canvas class="background-canvas"></canvas><main class="post-content"><nav class="post-nav"><div class="post-nav-left"><a href="/">‚Üê Home</a></div><div class="post-nav-right"><div class="animation-controls"><button class="animation-toggle" aria-label="Disable background animation">Disable Animation</button><button class="animation-speed" aria-label="Current speed: Medium">Speed: <!-- -->Medium</button></div></div></nav><article><h1>Symbolic Regression with Superposition on HVM3</h1><div class="post-meta"><time>2024-12-06</time></div><div class="markdown-content"><p>I have been working on a symbolic regression algorithm implementation on HVM. Let&#39;s start out with a motivation. Why try to reimplement symbolic regression, and what is this to begin with?</p>
<h2>Symbolic Regression</h2>
<p>As the name states it is a regression analysis, and in this particular case we want to find the best fit for a dataset. For example, consider the following data:
<code>x = [1, 2, 3, 4, 5] y = [2, 4, 6, 8, 10]</code></p>
<p>It is reasonable to say that the equation that &quot;commands&quot; this dataset is <code>y = 2x</code>. But how did you conclude this? You found a pattern between the y set and the x set. There are many ways of doing this in math, and symbolic regression is one of them.</p>
<p>What is different in this approach is that we are searching the space of mathematical expressions to find the model that best fits our data, both in terms of accuracy and simplicity. </p>
<p>To search things, we have to define a &quot;grammar&quot;, a set of symbols that define our equations. For example, suppose in our universe, we only have the opeartion of addition and we also have only the constants 1 and 2. How many equations can we enumerate?</p>
<p>Well, there are 6 of them:</p>
<ul>
<li><code>y = 1</code></li>
<li><code>y = 2</code> </li>
<li><code>y = 1 + 1</code></li>
<li><code>y = 1 + 2</code></li>
<li><code>y = 2 + 1</code></li>
<li><code>y = 2 + 2</code></li>
</ul>
<p>As you may imagine, if in a set with one operator and two constants we have 6 equations, and considering that the set of real numbers is infinite, the set of possible equations is also infinite right? So how to tackle this? Well, a good idea is to shrink this problem and solve a smaller instance first. So let&#39;s try the naive approach using a small set of symbols and constants.</p>
<p>Consider only the operators addition (+) and product (*), and the constants 1, 2, 3 and 4. And for now, we&#39;re using Haskell instead of HVM.</p>
<p>When doing this, my first thought was: how to model the equation? The most obvious answer (for me) was that the equation is a like an AST (abstract syntax tree). So let&#39;s model it like this:</p>
<ul>
<li>A tree is formed by either a leaf (that holds a constant), or a leaf that is a variable, or a node which contains the operator (we&#39;re using a string for now) and two other children nodes (considering that our operators are binary for now).</li>
</ul>
<p>In haskell:</p>
<pre><code class="language-hs">data Tree = Leaf Int | Var | Node String Tree Tree deriving (Show, Eq)
</code></pre>
<p>And my second thought was on how to enumerate every possibility of a tree given a depth. Turns out that the code is very simple, we simply map the 0 level as being the constants + a variable and the other levels being a combination of every operator with its down level. It looks like this:</p>
<pre><code>enumerateTrees :: Int -&gt; [Tree]
enumerateTrees maxDepth = concatMap generateTrees [0..maxDepth]
  where
    generateTrees :: Int -&gt; [Tree]
    generateTrees 0 = map Leaf constants ++ [Var]
    generateTrees depth =
        [Node op left right |
         op &lt;- operators,
         left &lt;- generateTrees (depth - 1),
         right &lt;- generateTrees (depth - 1)]
</code></pre>
<p>We also need a way to evaluate a certain tree, otherwise we can&#39;t check either our equation is correct or not. We also need a way to measure how far (or close) are we from the correct equation. </p>
<p>To do the evaluation, we simply need to translate our op strings to real operations and recursively eval the tree (where we receive a value for x, the variable).</p>
<pre><code>evaluateTree :: Tree -&gt; Int -&gt; Int
evaluateTree (Leaf n) _ = n
evaluateTree Var x = x
evaluateTree (Node op left right) x =
    case op of
        &quot;+&quot; -&gt; leftVal + rightVal
        &quot;*&quot; -&gt; leftVal * rightVal
        _ -&gt; 0
    where
        leftVal = evaluateTree left x
        rightVal = evaluateTree right x
</code></pre>
<p>For the fitness part, we will be using Mean Squared Error, which is basically the distance from our prediction to the correct prediction, but squared (to avoid negative values).</p>
<pre><code>fitness :: Tree -&gt; Data -&gt; Float
fitness tree data&#39; =
    let mse = sum [(fromIntegral (evaluateTree tree x) - fromIntegral y) ^ 2 | (x, y) &lt;- data&#39;]
    in mse / fromIntegral (length data&#39;)
</code></pre>
<p>Note that the <code>Data</code> type is simply a list of tuples <code>(x, y)</code>. So now, we want to minimize the MSE to find the best fit to the equation given by the dataset. The correct equation would have <code>MSE = 0</code>, since the distance from the predicted points and the actual points is zero.</p>
<p>Now, we need to actually run the symbolic regression, by finding the lowest fit. We basically loop through all enumerated trees up to a given depth and get the best one:</p>
<pre><code class="language-hs">symbolicRegression :: Data -&gt; Int -&gt; Int -&gt; Tree
symbolicRegression data&#39; maxDepth maxTrees =
    let trees = take maxTrees $ enumerateTrees maxDepth
        bestTree = minimumBy (comparing (\t -&gt; fitness t data&#39;)) trees
    in bestTree
</code></pre>
<p>And that&#39;s it, we now have a working symbolic regression algorithm that will search the mathematical space to find the equation that best models the dataset. It is easy to extend this with more operators, constants and so on.</p>
<p>But this is not quite performant right? Indeed, if you try to add a lot of constants or model a very complex equation that demands a tree with a high depth, it will take a lot of time (and maybe it won&#39;t find the exact fit nor a good one).</p>
<p>There are a lot of improvements and research in this kind of algorithm, as recombining equations using genetic programming, utilizing Bayesian methods and neural networks and so on. But before complicating this, let&#39;s discuss what&#39;s the role of HVM in this context.</p>
<h2>HVM</h2>
<p>HVM is a parallel runtime, a compiler and evaluator for high-level languages that implements optimal reduction through the concurrent computation model of Interaction Combinators.</p>
<p>Diving deep into theory, you&#39;ll reach some topics such as optimal reduction of lambda calculus, how to share computations, orders of evaluation and so on. But this is a little bit more complex than it looks, so I&#39;ll discuss it in another article. The point is that HVM achieves optimal evaluation.</p>
<p>In addition to this, in HVM there is the concept of <em>superposition</em>, that is the main reason of why we&#39;re going to write symbolic regression on HVM. Let&#39;s understand it through a simple example.</p>
<p>Suppose you want to factorize prime numbers in the most obvious way possible. We are just going to loop and test each factor untill we find a combination that results in the number we want. And let&#39;s do this in binary. (The code from this example was written by @VictorTaelin and its reference is in the end of the article).</p>
<p>First of all, we define the type of a bitstring:</p>
<p><code>data Bin { #O{pred} #I{pred} #E }</code></p>
<p>Which is a very common recursive data type. For example, <code>#O{#I{#E}}</code> is a valid bitstring that represents <code>O1</code>. Now, if <code>P</code> is the number we want to factorize we can define this factorization as: there are two numbers <code>x</code> and <code>y</code> that when multiplied result in <code>P</code>, where <code>x</code> and <code>y</code> are the factors. Therefore, <code>x * y = P</code>. So, we need to define multiplication, equality and other helper functions that are needed (increment, add, concat).</p>
<p>I will skip these implementations because they are not different from other functional languages. We will focus on the superposition concept and enumerating binary strings. So assume now we have all the operations we need. Now, the next step in other languages would be to loop through the numbers and check if their multiplication match. But here we will follow a different path.</p>
<p>We want to enumerate all possible bitstrings up to a given length. So for example, if we choose length 2, we have:</p>
<ul>
<li><code>01</code></li>
<li><code>00</code></li>
<li><code>11</code></li>
<li><code>10</code>
(ignoring the #end constructor)</li>
</ul>
<p>We always have <code>2^n</code> possibilities where <code>n</code> is the length given. The code to do this looks like:</p>
<pre><code>// Enums all Bins of given size (label 1)
@all1(s) = ~s{
  0: #E
  p: !&amp;1{p0 p1}=p &amp;1{ #O{@all1(p0)} #I{@all1(p1)} }
}
</code></pre>
<p>You may not be familliar with the syntax, but it&#39;s nothing too fancy. <code>@</code> defines a function called all1 that receives an argument <code>s</code> (the size of the bitstring. <code>~s</code> is a <code>match</code> on the size, which is a native number.</p>
<p>If <code>s</code> is zero, we reached the end of the bitstring, so simply return the end constructor. Otherwise, we:</p>
<ul>
<li>Duplicate p</li>
<li>Create a superposition where one side branches to <code>#O</code> and the other branches to <code>#I</code>.</li>
</ul>
<p>We have to duplicate <code>p</code> because HVM is linear, which mean we can only use a variable once. <code>!&amp;label{}</code> is just the syntax for duplicating. The label is important, but we&#39;re going to discuss it more later.</p>
<p>Now we create the superposition of all bitstrings. Let&#39;s take a look at the output of calling <code>@all1(2)</code> when running the file with <code>hvml run enum_primes.hvml</code>:</p>
<p><code>&amp;1{#0{&amp;1{#0{#2{}} #1{#2{}}}} #1{&amp;1{#0{#2{}} #1{#2{}}}}}</code></p>
<p>This is not very readable, but everytime you see a <code>&amp;</code> we are branching into a superposition. <code>#0 #1 and #2</code> are just the format HVM uses to the constructors, so <code>#O</code> is <code>#0</code>, <code>#I</code> is <code>#1</code> and so on. </p>
<p>Fortunately, we have a way to <em>collapse</em> the superpositions and see the all the bitstrings generated individually. The collapse algorithm is also very interesting, and a topic to another article.</p>
<pre><code>#0{#0{#2{}}}
#0{#1{#2{}}}
#1{#0{#2{}}}
#1{#1{#2{}}}
</code></pre>
<p>If you look closer, you&#39;ll see these 4 bitstrings are the ones i enumerated earlier. Cool right? So, basically what we are doing when opening an <code>&amp;{}</code> block is opening a superposition of two values (that can be composite as you can see), which means we are branching in two values. One universe goes to the <code>I</code> branch, and the other one goes to the <code>O</code> branch. But why is this so useful? Well, it is useful because this superposed values <em>share computation</em>. So for example, if we have this operation:
<code>(+ (* 2 3) &amp;{2 3})</code></p>
<p>The result when running with collapse mode is both 8 and 9, and they shared the <code>(* 2 3)</code> computation.</p>
<p>Another interesting property in the case of our binary data structure is the lazy abortion of useless universes. What does this mean? Well, I&#39;m going to use a similar example from <a href="https://gist.github.com/VictorTaelin/93c327e5b4e752b744d7798687977f8a">this article</a>, also from Victor Taelin.</p>
<p>Suppose we want to solve the following equation:</p>
<p><code>x + 1 == 3</code> or in binary, <code>x + 0b001 == 0b101</code></p>
<p>Typically, we will decrement 1 from both sides of the equation and find the predecessor of 0b101. With superpositions, we do it like (simplified syntax):</p>
<pre><code>x = {E {(O X) (I X)}}
y = (I ( O ( I E)))

// solving x + 1 == y
main = 
  let goal = (equal (add x (I E)) Y)
  (Collapse (if goal { X } else { * }))
</code></pre>
<p>This will kind of &quot;fork&quot; the HVM runtime into infinite universes, one for each value of <code>x</code>, and return the first one that satifies that equation. Then, in that &quot;universe&quot;, we print the value of <code>x</code>. In any other runtime, doing this would be exponential on the length <code>n</code> of the bitstring, as there are <code>2^n</code> bitstrings to consider, as stated before. However, this works and it is quadratic, because of the lazy abortion. An immense amount of shared work across all of those universes, and optimal lazy evaluation quickly aborts any universe where the equation isnt satisfied.</p>
<p>Reference: <a href="https://gist.github.com/VictorTaelin/9061306220929f04e7e6980f23ade615">https://gist.github.com/VictorTaelin/9061306220929f04e7e6980f23ade615</a></p>
<p>Now that we understand the concept of superposition, let&#39;s continue with the symbolic regression example. Let&#39;s model the same tree data type.</p>
<pre><code>data Tree {
  #Leaf{val}
  #Var{}
  #Node{op lft rgt}
}
</code></pre>
<p>Pretty much the same as the haskell implementation. I&#39;ll let some other constructors we will use below:</p>
<pre><code>data Pair {
  #Pair{fst snd}
}

data List {
  #Nil
  #Cons{head tail}
}

data Operators {
  #Plus
  #Mul
  #Min
  #Max
  #Mod
}
</code></pre>
<p>Since the enumeration part is the funniest part, i&#39;ll let it to the end. Let&#39;s define evaluation of a tree and fit first:</p>
<pre><code>@eval_tree(tree x) = ~tree !x {
  #Leaf{val}: val
  #Var: x
  #Node{op lft rgt}: 
    !&amp;0{x0 x1}=x
    !lftval = @eval_tree(lft x0)
    !rgtval = @eval_tree(rgt x1)
    ~op !lftval !rgtval {
      #Plus: (+ lftval rgtval) 
      #Mul: (* lftval rgtval)
      #Min: @min(lftval rgtval)
      #Max: @max(lftval rgtval)
      #Mod: (% lftval rgtval)
    } 
}
</code></pre>
<p>The syntax may look scary, but it is not that complicated. We are simply matching the tree constructors (and the !x syntax allows us to use x in every branch). If it is a leaf, the evaluation is simply the leaf value. If a var, the var value. If it is a node, we apply the node operation with the recursed evaluated left and right sides of the node.</p>
<pre><code>@mse(tree data acc acclen) = ~data !acc !tree !acclen {
  #Nil: #Pair{acc acclen}
  #Cons{h t}: 
    ~h {
      #Pair{x y}: 
        !&amp;4{t1 t2}=tree
        !pred_y  = @eval_tree(t1  x)
        !mse_val = (- pred_y y)
        !&amp;5{msev0 msev1}=mse_val
        !mse_sqrd = (* msev0 msev1)
        @mse(t2 t (+ acc mse_sqrd) (+ acclen 1)) 
    }
}

// where data is a list of pairs x, y that is the generated dataset
@fit(tree data) =
  !mselen = @mse(tree data 0 0)
  ~mselen {
    #Pair{mse len}: (/ mse len)
  }
</code></pre>
<p>For the fit implementation, we need the mse calculation. We&#39;re basically predicting y, calculating the metric and returning a pair with the mse and the length of the tree. The <code>!&amp;{}</code> syntax is explicit duplication, as used before.</p>
<p>And that&#39;s it for our measures and evaluation. Now we need to enumerate all the trees and finally run symbolic regression.</p>
<p>First of all, let&#39;s enumerate all operators possible:</p>
<p><code>@all_oper(&amp;L) = @SUP(&amp;L #Plus @SUP(&amp;L #Mul @SUP(&amp;L #Min @SUP(&amp;L #Max #Mod))))</code></p>
<p>Wait, but why are we using this different syntax for superpositions now? Well, if you take a closer look, we are receiving an argument. This argument is a label. This <code>@SUP</code> native function receives a label and the two things we want to superpose. It is the same as using the <code>&amp;{}</code> syntax, but now we can pass a label that we receive as an argument (and cloning it without having to manually do it for every call). The reason we need the same label is because we need the right and left branches of our tree to have different labels, otherwise we will not enumerate as many trees as we want.</p>
<p>An explanation to that comes from the <a href="https://discord.com/channels/912426566838013994/915345481675186197/1311434500911403109">Higher Order Co discord</a>. Therefore, we need to use dynamic sups and fork the labels (doing some label arithmetic). We will see this in work in the tree enumerator.</p>
<p>Let&#39;s also enumerate some constants possibilities:</p>
<pre><code>@consts = [#Var #Leaf{4} #Leaf{5}]

@all_consts(&amp;L c_list) = ~c_list {
  #Nil: *
  #Cons{h t}: @SUP(&amp;L h @all_consts(&amp;L t))
}
</code></pre>
<p>Here we are doing basically the same thing as in the operators superposition.</p>
<p>Now we have all set to enumerate the tree:</p>
<pre><code>@all(s l) = ~s !l {
  0:
    @all_consts(l @consts)
  p:
    !&amp;0{p p0}=p
    !&amp;0{p p1}=p
    !&amp;0{p p2}=p
    !&amp;0{p p3}=p

    !&amp;0{l l0}=l
    !&amp;0{l l1}=l
    !&amp;0{l l2}=l
    !&amp;0{l l3}=l
    !&amp;0{l l4}=l
    !&amp;0{l l5}=l
    !&amp;0{l l6}=l

    ! lL = (+ (* l0 2) 0)
    ! lR = (+ (* l1 2) 1)
    
    !all_oper = @all_oper(l6)

    @SUP(l2 #Node{all_oper @all(p1 (+ (* l3 2) 0)) @all(p2 (+ (* l4 2) 1))} @all_consts(l5 @consts))
}
</code></pre>
<p>We are enumerating all trees up to a given depth. When depth is zero, we are in the &quot;leaf level&quot;. So, we basically can have all constants in that level. Then, in the recursive level, we clone p a bunch of times (because we need to use it to the recursive calls). Then we clone <code>l</code>, which is the label for the current branch (because we use it to calculate the next labels and to label the current level&#39;s and branch constants and operators. Then, <code>lL</code> and <code>lR</code> are the new left and right branches for the next level. Then, we create a big superposition with all the possible operators, recursively call tree enumeration with the new left label, same for the right side, and also all the constants since the right and left side of a node can be a var or constants.</p>
<p>And now we have all the possible trees! Let&#39;s run a simple example with <code>depth=1</code> and collapse:</p>
<pre><code>&quot;#Var{}&quot;
&quot;#Leaf{4}&quot;
&quot;#Node{#Plus #Var{} #Var{}}&quot;
&quot;#Leaf{5}&quot;
&quot;#Node{#Plus #Var{} #Leaf{4}}&quot;
&quot;#Node{#Plus #Leaf{4} #Var{}}&quot;
&quot;#Node{#Mul #Var{} #Var{}}&quot;
&quot;#Node{#Plus #Var{} #Leaf{5}}&quot;
&quot;#Node{#Plus #Leaf{4} #Leaf{4}}&quot;
&quot;#Node{#Plus #Leaf{5} #Var{}}&quot;
&quot;#Node{#Mul #Var{} #Leaf{4}}&quot;
&quot;#Node{#Mul #Leaf{4} #Var{}}&quot;
&quot;#Node{#Min #Var{} #Var{}}&quot;
&quot;#Node{#Plus #Leaf{4} #Leaf{5}}&quot;
&quot;#Node{#Plus #Leaf{5} #Leaf{4}}&quot;
&quot;#Node{#Mul #Var{} #Leaf{5}}&quot;
&quot;#Node{#Mul #Leaf{4} #Leaf{4}}&quot;
&quot;#Node{#Mul #Leaf{5} #Var{}}&quot;
&quot;#Node{#Min #Var{} #Leaf{4}}&quot;
&quot;#Node{#Min #Leaf{4} #Var{}}&quot;
&quot;#Node{#Max #Var{} #Var{}}&quot;
&quot;#Node{#Mod #Var{} #Var{}}&quot;
&quot;#Node{#Plus #Leaf{5} #Leaf{5}}&quot;
&quot;#Node{#Mul #Leaf{4} #Leaf{5}}&quot;
&quot;#Node{#Mul #Leaf{5} #Leaf{4}}&quot;
&quot;#Node{#Min #Var{} #Leaf{5}}&quot;
&quot;#Node{#Min #Leaf{4} #Leaf{4}}&quot;
&quot;#Node{#Min #Leaf{5} #Var{}}&quot;
&quot;#Node{#Max #Var{} #Leaf{4}}&quot;
&quot;#Node{#Max #Leaf{4} #Var{}}&quot;
&quot;#Node{#Mod #Var{} #Leaf{4}}&quot;
&quot;#Node{#Mod #Leaf{4} #Var{}}&quot;
&quot;#Node{#Mul #Leaf{5} #Leaf{5}}&quot;
&quot;#Node{#Min #Leaf{4} #Leaf{5}}&quot;
&quot;#Node{#Min #Leaf{5} #Leaf{4}}&quot;
&quot;#Node{#Max #Var{} #Leaf{5}}&quot;
&quot;#Node{#Max #Leaf{4} #Leaf{4}}&quot;
&quot;#Node{#Max #Leaf{5} #Var{}}&quot;
&quot;#Node{#Mod #Var{} #Leaf{5}}&quot;
&quot;#Node{#Mod #Leaf{4} #Leaf{4}}&quot;
&quot;#Node{#Mod #Leaf{5} #Var{}}&quot;
&quot;#Node{#Min #Leaf{5} #Leaf{5}}&quot;
&quot;#Node{#Max #Leaf{4} #Leaf{5}}&quot;
&quot;#Node{#Max #Leaf{5} #Leaf{4}}&quot;
&quot;#Node{#Mod #Leaf{4} #Leaf{5}}&quot;
&quot;#Node{#Mod #Leaf{5} #Leaf{4}}&quot;
&quot;#Node{#Max #Leaf{5} #Leaf{5}}&quot;
&quot;#Node{#Mod #Leaf{5} #Leaf{5}}&quot;
</code></pre>
<p>I double checked that we have all the possibilities. The tree with depth 2 is very big, but if you wanna see it, give it a try. The gist with the complete code will be in the end of this article.</p>
<p>Now, we just need to tell hvm to collapse and print the result if the measure of fit is equal 0 (we are looking for exact matches). And this is simpler than it is in haskell:</p>
<pre><code>@X = @all(1 1)

@main =   
  !data = @data(10 #Nil 1)
  @if( (== @fit(@X data) 0) @do_show_tree(@X) *)
</code></pre>
<p>Simple, huh? We simply defined X as all the trees of depth 1 and said that for a given data, show us the result where the fit is 0. And this works! And it saves a lot of computation work by sharing.</p>
<p>Assume that @data simulates the equation <code>y = x * x</code>. Running our enumerator, we get:</p>
<p><code>&quot;#Node{#Mul #Var{} #Var{}}&quot;</code></p>
<p>Which is right!</p>
<p>That&#39;s it for today, we understood how superposition works and event wrote a very simple (but cool) example on symbolic regression. I&#39;ll leave some links and references below:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Symbolic_regression">Symbolic Regression</a></li>
<li><a href="https://github.com/HigherOrderCO/HVM3">HVM3</a></li>
<li><a href="https://higherorderco.com/">Higher Order CO</a></li>
<li><a href="https://x.com/VictorTaelin">Victor Taelin</a></li>
<li><a href="https://gist.github.com/VictorTaelin/9061306220929f04e7e6980f23ade615">SAT solver via superpositions</a></li>
</ul>
</div></article></main><footer><div class="footer-content"><p><a href="https://github.com/Lorenzobattistela" target="_blank" rel="noopener noreferrer">GitHub</a></p></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"superpositions","contentHtml":"\u003cp\u003eI have been working on a symbolic regression algorithm implementation on HVM. Let\u0026#39;s start out with a motivation. Why try to reimplement symbolic regression, and what is this to begin with?\u003c/p\u003e\n\u003ch2\u003eSymbolic Regression\u003c/h2\u003e\n\u003cp\u003eAs the name states it is a regression analysis, and in this particular case we want to find the best fit for a dataset. For example, consider the following data:\n\u003ccode\u003ex = [1, 2, 3, 4, 5] y = [2, 4, 6, 8, 10]\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eIt is reasonable to say that the equation that \u0026quot;commands\u0026quot; this dataset is \u003ccode\u003ey = 2x\u003c/code\u003e. But how did you conclude this? You found a pattern between the y set and the x set. There are many ways of doing this in math, and symbolic regression is one of them.\u003c/p\u003e\n\u003cp\u003eWhat is different in this approach is that we are searching the space of mathematical expressions to find the model that best fits our data, both in terms of accuracy and simplicity. \u003c/p\u003e\n\u003cp\u003eTo search things, we have to define a \u0026quot;grammar\u0026quot;, a set of symbols that define our equations. For example, suppose in our universe, we only have the opeartion of addition and we also have only the constants 1 and 2. How many equations can we enumerate?\u003c/p\u003e\n\u003cp\u003eWell, there are 6 of them:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ey = 1\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ey = 2\u003c/code\u003e \u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ey = 1 + 1\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ey = 1 + 2\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ey = 2 + 1\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ey = 2 + 2\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs you may imagine, if in a set with one operator and two constants we have 6 equations, and considering that the set of real numbers is infinite, the set of possible equations is also infinite right? So how to tackle this? Well, a good idea is to shrink this problem and solve a smaller instance first. So let\u0026#39;s try the naive approach using a small set of symbols and constants.\u003c/p\u003e\n\u003cp\u003eConsider only the operators addition (+) and product (*), and the constants 1, 2, 3 and 4. And for now, we\u0026#39;re using Haskell instead of HVM.\u003c/p\u003e\n\u003cp\u003eWhen doing this, my first thought was: how to model the equation? The most obvious answer (for me) was that the equation is a like an AST (abstract syntax tree). So let\u0026#39;s model it like this:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA tree is formed by either a leaf (that holds a constant), or a leaf that is a variable, or a node which contains the operator (we\u0026#39;re using a string for now) and two other children nodes (considering that our operators are binary for now).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn haskell:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-hs\"\u003edata Tree = Leaf Int | Var | Node String Tree Tree deriving (Show, Eq)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd my second thought was on how to enumerate every possibility of a tree given a depth. Turns out that the code is very simple, we simply map the 0 level as being the constants + a variable and the other levels being a combination of every operator with its down level. It looks like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eenumerateTrees :: Int -\u0026gt; [Tree]\nenumerateTrees maxDepth = concatMap generateTrees [0..maxDepth]\n  where\n    generateTrees :: Int -\u0026gt; [Tree]\n    generateTrees 0 = map Leaf constants ++ [Var]\n    generateTrees depth =\n        [Node op left right |\n         op \u0026lt;- operators,\n         left \u0026lt;- generateTrees (depth - 1),\n         right \u0026lt;- generateTrees (depth - 1)]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe also need a way to evaluate a certain tree, otherwise we can\u0026#39;t check either our equation is correct or not. We also need a way to measure how far (or close) are we from the correct equation. \u003c/p\u003e\n\u003cp\u003eTo do the evaluation, we simply need to translate our op strings to real operations and recursively eval the tree (where we receive a value for x, the variable).\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eevaluateTree :: Tree -\u0026gt; Int -\u0026gt; Int\nevaluateTree (Leaf n) _ = n\nevaluateTree Var x = x\nevaluateTree (Node op left right) x =\n    case op of\n        \u0026quot;+\u0026quot; -\u0026gt; leftVal + rightVal\n        \u0026quot;*\u0026quot; -\u0026gt; leftVal * rightVal\n        _ -\u0026gt; 0\n    where\n        leftVal = evaluateTree left x\n        rightVal = evaluateTree right x\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor the fitness part, we will be using Mean Squared Error, which is basically the distance from our prediction to the correct prediction, but squared (to avoid negative values).\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efitness :: Tree -\u0026gt; Data -\u0026gt; Float\nfitness tree data\u0026#39; =\n    let mse = sum [(fromIntegral (evaluateTree tree x) - fromIntegral y) ^ 2 | (x, y) \u0026lt;- data\u0026#39;]\n    in mse / fromIntegral (length data\u0026#39;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that the \u003ccode\u003eData\u003c/code\u003e type is simply a list of tuples \u003ccode\u003e(x, y)\u003c/code\u003e. So now, we want to minimize the MSE to find the best fit to the equation given by the dataset. The correct equation would have \u003ccode\u003eMSE = 0\u003c/code\u003e, since the distance from the predicted points and the actual points is zero.\u003c/p\u003e\n\u003cp\u003eNow, we need to actually run the symbolic regression, by finding the lowest fit. We basically loop through all enumerated trees up to a given depth and get the best one:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-hs\"\u003esymbolicRegression :: Data -\u0026gt; Int -\u0026gt; Int -\u0026gt; Tree\nsymbolicRegression data\u0026#39; maxDepth maxTrees =\n    let trees = take maxTrees $ enumerateTrees maxDepth\n        bestTree = minimumBy (comparing (\\t -\u0026gt; fitness t data\u0026#39;)) trees\n    in bestTree\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd that\u0026#39;s it, we now have a working symbolic regression algorithm that will search the mathematical space to find the equation that best models the dataset. It is easy to extend this with more operators, constants and so on.\u003c/p\u003e\n\u003cp\u003eBut this is not quite performant right? Indeed, if you try to add a lot of constants or model a very complex equation that demands a tree with a high depth, it will take a lot of time (and maybe it won\u0026#39;t find the exact fit nor a good one).\u003c/p\u003e\n\u003cp\u003eThere are a lot of improvements and research in this kind of algorithm, as recombining equations using genetic programming, utilizing Bayesian methods and neural networks and so on. But before complicating this, let\u0026#39;s discuss what\u0026#39;s the role of HVM in this context.\u003c/p\u003e\n\u003ch2\u003eHVM\u003c/h2\u003e\n\u003cp\u003eHVM is a parallel runtime, a compiler and evaluator for high-level languages that implements optimal reduction through the concurrent computation model of Interaction Combinators.\u003c/p\u003e\n\u003cp\u003eDiving deep into theory, you\u0026#39;ll reach some topics such as optimal reduction of lambda calculus, how to share computations, orders of evaluation and so on. But this is a little bit more complex than it looks, so I\u0026#39;ll discuss it in another article. The point is that HVM achieves optimal evaluation.\u003c/p\u003e\n\u003cp\u003eIn addition to this, in HVM there is the concept of \u003cem\u003esuperposition\u003c/em\u003e, that is the main reason of why we\u0026#39;re going to write symbolic regression on HVM. Let\u0026#39;s understand it through a simple example.\u003c/p\u003e\n\u003cp\u003eSuppose you want to factorize prime numbers in the most obvious way possible. We are just going to loop and test each factor untill we find a combination that results in the number we want. And let\u0026#39;s do this in binary. (The code from this example was written by @VictorTaelin and its reference is in the end of the article).\u003c/p\u003e\n\u003cp\u003eFirst of all, we define the type of a bitstring:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003edata Bin { #O{pred} #I{pred} #E }\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eWhich is a very common recursive data type. For example, \u003ccode\u003e#O{#I{#E}}\u003c/code\u003e is a valid bitstring that represents \u003ccode\u003eO1\u003c/code\u003e. Now, if \u003ccode\u003eP\u003c/code\u003e is the number we want to factorize we can define this factorization as: there are two numbers \u003ccode\u003ex\u003c/code\u003e and \u003ccode\u003ey\u003c/code\u003e that when multiplied result in \u003ccode\u003eP\u003c/code\u003e, where \u003ccode\u003ex\u003c/code\u003e and \u003ccode\u003ey\u003c/code\u003e are the factors. Therefore, \u003ccode\u003ex * y = P\u003c/code\u003e. So, we need to define multiplication, equality and other helper functions that are needed (increment, add, concat).\u003c/p\u003e\n\u003cp\u003eI will skip these implementations because they are not different from other functional languages. We will focus on the superposition concept and enumerating binary strings. So assume now we have all the operations we need. Now, the next step in other languages would be to loop through the numbers and check if their multiplication match. But here we will follow a different path.\u003c/p\u003e\n\u003cp\u003eWe want to enumerate all possible bitstrings up to a given length. So for example, if we choose length 2, we have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e01\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e00\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e11\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e10\u003c/code\u003e\n(ignoring the #end constructor)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe always have \u003ccode\u003e2^n\u003c/code\u003e possibilities where \u003ccode\u003en\u003c/code\u003e is the length given. The code to do this looks like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e// Enums all Bins of given size (label 1)\n@all1(s) = ~s{\n  0: #E\n  p: !\u0026amp;1{p0 p1}=p \u0026amp;1{ #O{@all1(p0)} #I{@all1(p1)} }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou may not be familliar with the syntax, but it\u0026#39;s nothing too fancy. \u003ccode\u003e@\u003c/code\u003e defines a function called all1 that receives an argument \u003ccode\u003es\u003c/code\u003e (the size of the bitstring. \u003ccode\u003e~s\u003c/code\u003e is a \u003ccode\u003ematch\u003c/code\u003e on the size, which is a native number.\u003c/p\u003e\n\u003cp\u003eIf \u003ccode\u003es\u003c/code\u003e is zero, we reached the end of the bitstring, so simply return the end constructor. Otherwise, we:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDuplicate p\u003c/li\u003e\n\u003cli\u003eCreate a superposition where one side branches to \u003ccode\u003e#O\u003c/code\u003e and the other branches to \u003ccode\u003e#I\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe have to duplicate \u003ccode\u003ep\u003c/code\u003e because HVM is linear, which mean we can only use a variable once. \u003ccode\u003e!\u0026amp;label{}\u003c/code\u003e is just the syntax for duplicating. The label is important, but we\u0026#39;re going to discuss it more later.\u003c/p\u003e\n\u003cp\u003eNow we create the superposition of all bitstrings. Let\u0026#39;s take a look at the output of calling \u003ccode\u003e@all1(2)\u003c/code\u003e when running the file with \u003ccode\u003ehvml run enum_primes.hvml\u003c/code\u003e:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\u0026amp;1{#0{\u0026amp;1{#0{#2{}} #1{#2{}}}} #1{\u0026amp;1{#0{#2{}} #1{#2{}}}}}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThis is not very readable, but everytime you see a \u003ccode\u003e\u0026amp;\u003c/code\u003e we are branching into a superposition. \u003ccode\u003e#0 #1 and #2\u003c/code\u003e are just the format HVM uses to the constructors, so \u003ccode\u003e#O\u003c/code\u003e is \u003ccode\u003e#0\u003c/code\u003e, \u003ccode\u003e#I\u003c/code\u003e is \u003ccode\u003e#1\u003c/code\u003e and so on. \u003c/p\u003e\n\u003cp\u003eFortunately, we have a way to \u003cem\u003ecollapse\u003c/em\u003e the superpositions and see the all the bitstrings generated individually. The collapse algorithm is also very interesting, and a topic to another article.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#0{#0{#2{}}}\n#0{#1{#2{}}}\n#1{#0{#2{}}}\n#1{#1{#2{}}}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you look closer, you\u0026#39;ll see these 4 bitstrings are the ones i enumerated earlier. Cool right? So, basically what we are doing when opening an \u003ccode\u003e\u0026amp;{}\u003c/code\u003e block is opening a superposition of two values (that can be composite as you can see), which means we are branching in two values. One universe goes to the \u003ccode\u003eI\u003c/code\u003e branch, and the other one goes to the \u003ccode\u003eO\u003c/code\u003e branch. But why is this so useful? Well, it is useful because this superposed values \u003cem\u003eshare computation\u003c/em\u003e. So for example, if we have this operation:\n\u003ccode\u003e(+ (* 2 3) \u0026amp;{2 3})\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThe result when running with collapse mode is both 8 and 9, and they shared the \u003ccode\u003e(* 2 3)\u003c/code\u003e computation.\u003c/p\u003e\n\u003cp\u003eAnother interesting property in the case of our binary data structure is the lazy abortion of useless universes. What does this mean? Well, I\u0026#39;m going to use a similar example from \u003ca href=\"https://gist.github.com/VictorTaelin/93c327e5b4e752b744d7798687977f8a\"\u003ethis article\u003c/a\u003e, also from Victor Taelin.\u003c/p\u003e\n\u003cp\u003eSuppose we want to solve the following equation:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ex + 1 == 3\u003c/code\u003e or in binary, \u003ccode\u003ex + 0b001 == 0b101\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eTypically, we will decrement 1 from both sides of the equation and find the predecessor of 0b101. With superpositions, we do it like (simplified syntax):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ex = {E {(O X) (I X)}}\ny = (I ( O ( I E)))\n\n// solving x + 1 == y\nmain = \n  let goal = (equal (add x (I E)) Y)\n  (Collapse (if goal { X } else { * }))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will kind of \u0026quot;fork\u0026quot; the HVM runtime into infinite universes, one for each value of \u003ccode\u003ex\u003c/code\u003e, and return the first one that satifies that equation. Then, in that \u0026quot;universe\u0026quot;, we print the value of \u003ccode\u003ex\u003c/code\u003e. In any other runtime, doing this would be exponential on the length \u003ccode\u003en\u003c/code\u003e of the bitstring, as there are \u003ccode\u003e2^n\u003c/code\u003e bitstrings to consider, as stated before. However, this works and it is quadratic, because of the lazy abortion. An immense amount of shared work across all of those universes, and optimal lazy evaluation quickly aborts any universe where the equation isnt satisfied.\u003c/p\u003e\n\u003cp\u003eReference: \u003ca href=\"https://gist.github.com/VictorTaelin/9061306220929f04e7e6980f23ade615\"\u003ehttps://gist.github.com/VictorTaelin/9061306220929f04e7e6980f23ade615\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eNow that we understand the concept of superposition, let\u0026#39;s continue with the symbolic regression example. Let\u0026#39;s model the same tree data type.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edata Tree {\n  #Leaf{val}\n  #Var{}\n  #Node{op lft rgt}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePretty much the same as the haskell implementation. I\u0026#39;ll let some other constructors we will use below:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edata Pair {\n  #Pair{fst snd}\n}\n\ndata List {\n  #Nil\n  #Cons{head tail}\n}\n\ndata Operators {\n  #Plus\n  #Mul\n  #Min\n  #Max\n  #Mod\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSince the enumeration part is the funniest part, i\u0026#39;ll let it to the end. Let\u0026#39;s define evaluation of a tree and fit first:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@eval_tree(tree x) = ~tree !x {\n  #Leaf{val}: val\n  #Var: x\n  #Node{op lft rgt}: \n    !\u0026amp;0{x0 x1}=x\n    !lftval = @eval_tree(lft x0)\n    !rgtval = @eval_tree(rgt x1)\n    ~op !lftval !rgtval {\n      #Plus: (+ lftval rgtval) \n      #Mul: (* lftval rgtval)\n      #Min: @min(lftval rgtval)\n      #Max: @max(lftval rgtval)\n      #Mod: (% lftval rgtval)\n    } \n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe syntax may look scary, but it is not that complicated. We are simply matching the tree constructors (and the !x syntax allows us to use x in every branch). If it is a leaf, the evaluation is simply the leaf value. If a var, the var value. If it is a node, we apply the node operation with the recursed evaluated left and right sides of the node.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@mse(tree data acc acclen) = ~data !acc !tree !acclen {\n  #Nil: #Pair{acc acclen}\n  #Cons{h t}: \n    ~h {\n      #Pair{x y}: \n        !\u0026amp;4{t1 t2}=tree\n        !pred_y  = @eval_tree(t1  x)\n        !mse_val = (- pred_y y)\n        !\u0026amp;5{msev0 msev1}=mse_val\n        !mse_sqrd = (* msev0 msev1)\n        @mse(t2 t (+ acc mse_sqrd) (+ acclen 1)) \n    }\n}\n\n// where data is a list of pairs x, y that is the generated dataset\n@fit(tree data) =\n  !mselen = @mse(tree data 0 0)\n  ~mselen {\n    #Pair{mse len}: (/ mse len)\n  }\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor the fit implementation, we need the mse calculation. We\u0026#39;re basically predicting y, calculating the metric and returning a pair with the mse and the length of the tree. The \u003ccode\u003e!\u0026amp;{}\u003c/code\u003e syntax is explicit duplication, as used before.\u003c/p\u003e\n\u003cp\u003eAnd that\u0026#39;s it for our measures and evaluation. Now we need to enumerate all the trees and finally run symbolic regression.\u003c/p\u003e\n\u003cp\u003eFirst of all, let\u0026#39;s enumerate all operators possible:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e@all_oper(\u0026amp;L) = @SUP(\u0026amp;L #Plus @SUP(\u0026amp;L #Mul @SUP(\u0026amp;L #Min @SUP(\u0026amp;L #Max #Mod))))\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eWait, but why are we using this different syntax for superpositions now? Well, if you take a closer look, we are receiving an argument. This argument is a label. This \u003ccode\u003e@SUP\u003c/code\u003e native function receives a label and the two things we want to superpose. It is the same as using the \u003ccode\u003e\u0026amp;{}\u003c/code\u003e syntax, but now we can pass a label that we receive as an argument (and cloning it without having to manually do it for every call). The reason we need the same label is because we need the right and left branches of our tree to have different labels, otherwise we will not enumerate as many trees as we want.\u003c/p\u003e\n\u003cp\u003eAn explanation to that comes from the \u003ca href=\"https://discord.com/channels/912426566838013994/915345481675186197/1311434500911403109\"\u003eHigher Order Co discord\u003c/a\u003e. Therefore, we need to use dynamic sups and fork the labels (doing some label arithmetic). We will see this in work in the tree enumerator.\u003c/p\u003e\n\u003cp\u003eLet\u0026#39;s also enumerate some constants possibilities:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@consts = [#Var #Leaf{4} #Leaf{5}]\n\n@all_consts(\u0026amp;L c_list) = ~c_list {\n  #Nil: *\n  #Cons{h t}: @SUP(\u0026amp;L h @all_consts(\u0026amp;L t))\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere we are doing basically the same thing as in the operators superposition.\u003c/p\u003e\n\u003cp\u003eNow we have all set to enumerate the tree:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@all(s l) = ~s !l {\n  0:\n    @all_consts(l @consts)\n  p:\n    !\u0026amp;0{p p0}=p\n    !\u0026amp;0{p p1}=p\n    !\u0026amp;0{p p2}=p\n    !\u0026amp;0{p p3}=p\n\n    !\u0026amp;0{l l0}=l\n    !\u0026amp;0{l l1}=l\n    !\u0026amp;0{l l2}=l\n    !\u0026amp;0{l l3}=l\n    !\u0026amp;0{l l4}=l\n    !\u0026amp;0{l l5}=l\n    !\u0026amp;0{l l6}=l\n\n    ! lL = (+ (* l0 2) 0)\n    ! lR = (+ (* l1 2) 1)\n    \n    !all_oper = @all_oper(l6)\n\n    @SUP(l2 #Node{all_oper @all(p1 (+ (* l3 2) 0)) @all(p2 (+ (* l4 2) 1))} @all_consts(l5 @consts))\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe are enumerating all trees up to a given depth. When depth is zero, we are in the \u0026quot;leaf level\u0026quot;. So, we basically can have all constants in that level. Then, in the recursive level, we clone p a bunch of times (because we need to use it to the recursive calls). Then we clone \u003ccode\u003el\u003c/code\u003e, which is the label for the current branch (because we use it to calculate the next labels and to label the current level\u0026#39;s and branch constants and operators. Then, \u003ccode\u003elL\u003c/code\u003e and \u003ccode\u003elR\u003c/code\u003e are the new left and right branches for the next level. Then, we create a big superposition with all the possible operators, recursively call tree enumeration with the new left label, same for the right side, and also all the constants since the right and left side of a node can be a var or constants.\u003c/p\u003e\n\u003cp\u003eAnd now we have all the possible trees! Let\u0026#39;s run a simple example with \u003ccode\u003edepth=1\u003c/code\u003e and collapse:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026quot;#Var{}\u0026quot;\n\u0026quot;#Leaf{4}\u0026quot;\n\u0026quot;#Node{#Plus #Var{} #Var{}}\u0026quot;\n\u0026quot;#Leaf{5}\u0026quot;\n\u0026quot;#Node{#Plus #Var{} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Plus #Leaf{4} #Var{}}\u0026quot;\n\u0026quot;#Node{#Mul #Var{} #Var{}}\u0026quot;\n\u0026quot;#Node{#Plus #Var{} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Plus #Leaf{4} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Plus #Leaf{5} #Var{}}\u0026quot;\n\u0026quot;#Node{#Mul #Var{} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Mul #Leaf{4} #Var{}}\u0026quot;\n\u0026quot;#Node{#Min #Var{} #Var{}}\u0026quot;\n\u0026quot;#Node{#Plus #Leaf{4} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Plus #Leaf{5} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Mul #Var{} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Mul #Leaf{4} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Mul #Leaf{5} #Var{}}\u0026quot;\n\u0026quot;#Node{#Min #Var{} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Min #Leaf{4} #Var{}}\u0026quot;\n\u0026quot;#Node{#Max #Var{} #Var{}}\u0026quot;\n\u0026quot;#Node{#Mod #Var{} #Var{}}\u0026quot;\n\u0026quot;#Node{#Plus #Leaf{5} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Mul #Leaf{4} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Mul #Leaf{5} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Min #Var{} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Min #Leaf{4} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Min #Leaf{5} #Var{}}\u0026quot;\n\u0026quot;#Node{#Max #Var{} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Max #Leaf{4} #Var{}}\u0026quot;\n\u0026quot;#Node{#Mod #Var{} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Mod #Leaf{4} #Var{}}\u0026quot;\n\u0026quot;#Node{#Mul #Leaf{5} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Min #Leaf{4} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Min #Leaf{5} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Max #Var{} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Max #Leaf{4} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Max #Leaf{5} #Var{}}\u0026quot;\n\u0026quot;#Node{#Mod #Var{} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Mod #Leaf{4} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Mod #Leaf{5} #Var{}}\u0026quot;\n\u0026quot;#Node{#Min #Leaf{5} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Max #Leaf{4} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Max #Leaf{5} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Mod #Leaf{4} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Mod #Leaf{5} #Leaf{4}}\u0026quot;\n\u0026quot;#Node{#Max #Leaf{5} #Leaf{5}}\u0026quot;\n\u0026quot;#Node{#Mod #Leaf{5} #Leaf{5}}\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eI double checked that we have all the possibilities. The tree with depth 2 is very big, but if you wanna see it, give it a try. The gist with the complete code will be in the end of this article.\u003c/p\u003e\n\u003cp\u003eNow, we just need to tell hvm to collapse and print the result if the measure of fit is equal 0 (we are looking for exact matches). And this is simpler than it is in haskell:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@X = @all(1 1)\n\n@main =   \n  !data = @data(10 #Nil 1)\n  @if( (== @fit(@X data) 0) @do_show_tree(@X) *)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSimple, huh? We simply defined X as all the trees of depth 1 and said that for a given data, show us the result where the fit is 0. And this works! And it saves a lot of computation work by sharing.\u003c/p\u003e\n\u003cp\u003eAssume that @data simulates the equation \u003ccode\u003ey = x * x\u003c/code\u003e. Running our enumerator, we get:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\u0026quot;#Node{#Mul #Var{} #Var{}}\u0026quot;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eWhich is right!\u003c/p\u003e\n\u003cp\u003eThat\u0026#39;s it for today, we understood how superposition works and event wrote a very simple (but cool) example on symbolic regression. I\u0026#39;ll leave some links and references below:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Symbolic_regression\"\u003eSymbolic Regression\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/HigherOrderCO/HVM3\"\u003eHVM3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://higherorderco.com/\"\u003eHigher Order CO\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://x.com/VictorTaelin\"\u003eVictor Taelin\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gist.github.com/VictorTaelin/9061306220929f04e7e6980f23ade615\"\u003eSAT solver via superpositions\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","title":"Symbolic Regression with Superposition on HVM3","description":"Understanding HVM superpositions","date":"2024-12-06"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"superpositions"},"buildId":"ydYcoKj9y-mj7XR-0MT3a","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>