<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1.0" data-next-head=""/><meta name="author" content="Lorenzo Battistela" data-next-head=""/><link rel="icon" href="/favicon.ico" data-next-head=""/><title data-next-head=""></title><meta name="description" content="Understanding linear regression principles and math." data-next-head=""/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/966ef0bc2ff51cf6.css" as="style"/><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&amp;family=IBM+Plex+Sans:wght@400;500;600;700&amp;display=swap" rel="stylesheet"/><link rel="stylesheet" href="/_next/static/css/966ef0bc2ff51cf6.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-afbe389e5387c555.js" defer=""></script><script src="/_next/static/chunks/framework-d7f578ab3069408c.js" defer=""></script><script src="/_next/static/chunks/main-a5703b490a653431.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d35945f6f2ae940c.js" defer=""></script><script src="/_next/static/chunks/230-8a10a6030a242aaa.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-9da8c7586f67ce5f.js" defer=""></script><script src="/_next/static/7XdtTWnKkLqV613zeyCgO/_buildManifest.js" defer=""></script><script src="/_next/static/7XdtTWnKkLqV613zeyCgO/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="container"><canvas class="background-canvas"></canvas><main class="post-content"><nav class="post-nav"><div class="post-nav-left"><a href="/">← Home</a></div><div class="post-nav-right"><div class="animation-controls"><button class="animation-toggle" aria-label="Disable background animation">Disable Animation</button><button class="animation-speed" aria-label="Current speed: Medium">Speed: <!-- -->Medium</button></div></div></nav><article><h1>Understanding Linear Regression</h1><div class="post-meta"><time>2024-05-16</time></div><div class="markdown-content"><h1>Understanding Linear Regression</h1>
<p>Linear regression is such a common subject when learning data science, and it is a good starting point for most beginners to build a model. In general, most people use libraries like Tensorflow to get this done, but today we are going to understand step by step how does Linear Regression work. Dont get me wrong, this libraries are really useful. But doing it from scratch teaches us a lot more than we think (even if our linear regression performance is worse than the Tensorflow one). Lets dive into it now!</p>
<p>What is Linear Regression? Well, basically it is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables. For today, we will only look into simple linear regression, that is, when we have only one explanatory variable (or independent).</p>
<p>The relationship is modeled using linear predictor functions, and model parameters are estimated from the data (which makes it a linear model).</p>
<p>Linear regression focuses on conditional probability distribution of the response given the values of predictors (given two jointly distributed random variables X and Y, conditional probability distribution of Y given X is the probability of Y when X is known its value).</p>
<p>Therefore, a a model using linear regression assumes that the relationship between dependent and independent variable is <strong>linear</strong>. We say there is a regression relationship between X and Y when the mean of Y varies with X.</p>
<p>A simple way to think about this is height and weight. Consider a relationship between height (Y) and weight (X) of a group of people. Lets say that the mean height at a given height is</p>
<p><code>p(x) = 3x / 5 - 38</code></p>
<p>for all x &gt; 110.</p>
<p>The difference between an observed weight (real one) and mean weight at a given height provided by our formula is referred to as the error for that weight.</p>
<p>Therefore, we can said that the equation for linear regression is:</p>
<p><img src="/images/understanding-linear-regression/equation1.webp" alt="equation linreg"></p>
<p>where y is (in our example) the height of some individual, alpha is some parameter (represented in this case by negative 38 minus error), beta is another parameter (represented by 3/5) and the error (minus 38 minus alpha).</p>
<p>That said, to discover our linear relationship, we could measure the height of three individuals at each weight and apply linear regression to model the mean height as a function of weight using a straight line. The most popular way to estimate parameters is the least-squares estimator (LSE). Before understanding this, remember your high school math classes. In this equation, beta is the slope of the line, and alpha plus error is the interception point. Now lets continue with LSE.</p>
<p>The method of least squares estimates the solution of our equations by minimizing the sum of the squares of the residuals (where the residual is the difference between observed value and fitted value provided by model) made in the results of each individual equation.</p>
<p>The estimates are given by:</p>
<p><img src="/images/understanding-linear-regression/equation2.webp" alt="Figure 2"></p>
<p>Where</p>
<p><img src="/images/understanding-linear-regression/equation3.webp" alt="Figure 3"></p>
<p>The above equation are the points on the estimated regression line (called the fitted ones, or predicted). The estimates are given by:</p>
<p><img src="/images/understanding-linear-regression/equation4.webp" alt="Figure 4"> <br><img src="/images/understanding-linear-regression/equation5.webp" alt="Figure 5"></p>
<p>Where X and Y are means of samples x and y, and sx, sy are their standard deviation values, and r is their correlation coefficient.</p>
<p>LSE is really common because it is good for general error distributions, and this means that when we pick a random Y value from the conditional distribution Y|X, the LSEs will not be too high or too low (although they might deviate from the real values). However, this approach is sensitive to extreme values, or outliers from both sides. This means we have to clean outliers or evaluate them before applying LSE.</p>
<p>If you want to understand how this happens in code, I highly recommend taking a look on this repository:</p>
<p><a href="https://github.com/Lorenzobattistela/linear-regression?source=post_page-----d252ed669e90--------------------------------">GitHub - Lorenzobattistela/linear-regression</a></p>
<p>Here, I build a simple linear regression model from scratch, building all helpers and math functions needed for it to work. More instructions on how to run it on repository README.</p>
<p>Linear regression is a good way to start studying models and understanding the math behind it. It does not finish on simple LR, since we can have multivariable models. That said, that is a lot to explore and understand about it, and best part is that is easier to do by yourself!</p>
<p>Thanks for reading this article.</p>
<p>Best regards, Lorenzo.</p>
<p>References:</p>
<p>Altman, N., Krzywinski, M. Simple linear regression. <em>Nat Methods</em> <strong>12</strong>, 999–1000 (2015).</p>
<p><a href="https://en.wikipedia.org/wiki/Simple_linear_regression">https://en.wikipedia.org/wiki/Simple_linear_regression</a></p>
<p><a href="https://en.wikipedia.org/wiki/Linear_regression">https://en.wikipedia.org/wiki/Linear_regression</a></p>
<p><a href="https://en.wikipedia.org/wiki/Linear_least_squares">https://en.wikipedia.org/wiki/Linear_least_squares</a></p>
<p><a href="https://github.com/Lorenzobattistela/linear-regression">https://github.com/Lorenzobattistela/linear-regression</a></p>
</div></article></main><footer><div class="footer-content"><p><a href="https://github.com/Lorenzobattistela" target="_blank" rel="noopener noreferrer">GitHub</a></p></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"understanding-linear-regression","contentHtml":"\u003ch1\u003eUnderstanding Linear Regression\u003c/h1\u003e\n\u003cp\u003eLinear regression is such a common subject when learning data science, and it is a good starting point for most beginners to build a model. In general, most people use libraries like Tensorflow to get this done, but today we are going to understand step by step how does Linear Regression work. Dont get me wrong, this libraries are really useful. But doing it from scratch teaches us a lot more than we think (even if our linear regression performance is worse than the Tensorflow one). Lets dive into it now!\u003c/p\u003e\n\u003cp\u003eWhat is Linear Regression? Well, basically it is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables. For today, we will only look into simple linear regression, that is, when we have only one explanatory variable (or independent).\u003c/p\u003e\n\u003cp\u003eThe relationship is modeled using linear predictor functions, and model parameters are estimated from the data (which makes it a linear model).\u003c/p\u003e\n\u003cp\u003eLinear regression focuses on conditional probability distribution of the response given the values of predictors (given two jointly distributed random variables X and Y, conditional probability distribution of Y given X is the probability of Y when X is known its value).\u003c/p\u003e\n\u003cp\u003eTherefore, a a model using linear regression assumes that the relationship between dependent and independent variable is \u003cstrong\u003elinear\u003c/strong\u003e. We say there is a regression relationship between X and Y when the mean of Y varies with X.\u003c/p\u003e\n\u003cp\u003eA simple way to think about this is height and weight. Consider a relationship between height (Y) and weight (X) of a group of people. Lets say that the mean height at a given height is\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ep(x) = 3x / 5 - 38\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003efor all x \u0026gt; 110.\u003c/p\u003e\n\u003cp\u003eThe difference between an observed weight (real one) and mean weight at a given height provided by our formula is referred to as the error for that weight.\u003c/p\u003e\n\u003cp\u003eTherefore, we can said that the equation for linear regression is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/understanding-linear-regression/equation1.webp\" alt=\"equation linreg\"\u003e\u003c/p\u003e\n\u003cp\u003ewhere y is (in our example) the height of some individual, alpha is some parameter (represented in this case by negative 38 minus error), beta is another parameter (represented by 3/5) and the error (minus 38 minus alpha).\u003c/p\u003e\n\u003cp\u003eThat said, to discover our linear relationship, we could measure the height of three individuals at each weight and apply linear regression to model the mean height as a function of weight using a straight line. The most popular way to estimate parameters is the least-squares estimator (LSE). Before understanding this, remember your high school math classes. In this equation, beta is the slope of the line, and alpha plus error is the interception point. Now lets continue with LSE.\u003c/p\u003e\n\u003cp\u003eThe method of least squares estimates the solution of our equations by minimizing the sum of the squares of the residuals (where the residual is the difference between observed value and fitted value provided by model) made in the results of each individual equation.\u003c/p\u003e\n\u003cp\u003eThe estimates are given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/understanding-linear-regression/equation2.webp\" alt=\"Figure 2\"\u003e\u003c/p\u003e\n\u003cp\u003eWhere\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/understanding-linear-regression/equation3.webp\" alt=\"Figure 3\"\u003e\u003c/p\u003e\n\u003cp\u003eThe above equation are the points on the estimated regression line (called the fitted ones, or predicted). The estimates are given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/understanding-linear-regression/equation4.webp\" alt=\"Figure 4\"\u003e \u003cbr\u003e\u003cimg src=\"/images/understanding-linear-regression/equation5.webp\" alt=\"Figure 5\"\u003e\u003c/p\u003e\n\u003cp\u003eWhere X and Y are means of samples x and y, and sx, sy are their standard deviation values, and r is their correlation coefficient.\u003c/p\u003e\n\u003cp\u003eLSE is really common because it is good for general error distributions, and this means that when we pick a random Y value from the conditional distribution Y|X, the LSEs will not be too high or too low (although they might deviate from the real values). However, this approach is sensitive to extreme values, or outliers from both sides. This means we have to clean outliers or evaluate them before applying LSE.\u003c/p\u003e\n\u003cp\u003eIf you want to understand how this happens in code, I highly recommend taking a look on this repository:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Lorenzobattistela/linear-regression?source=post_page-----d252ed669e90--------------------------------\"\u003eGitHub - Lorenzobattistela/linear-regression\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHere, I build a simple linear regression model from scratch, building all helpers and math functions needed for it to work. More instructions on how to run it on repository README.\u003c/p\u003e\n\u003cp\u003eLinear regression is a good way to start studying models and understanding the math behind it. It does not finish on simple LR, since we can have multivariable models. That said, that is a lot to explore and understand about it, and best part is that is easier to do by yourself!\u003c/p\u003e\n\u003cp\u003eThanks for reading this article.\u003c/p\u003e\n\u003cp\u003eBest regards, Lorenzo.\u003c/p\u003e\n\u003cp\u003eReferences:\u003c/p\u003e\n\u003cp\u003eAltman, N., Krzywinski, M. Simple linear regression. \u003cem\u003eNat Methods\u003c/em\u003e \u003cstrong\u003e12\u003c/strong\u003e, 999–1000 (2015).\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Simple_linear_regression\"\u003ehttps://en.wikipedia.org/wiki/Simple_linear_regression\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Linear_regression\"\u003ehttps://en.wikipedia.org/wiki/Linear_regression\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Linear_least_squares\"\u003ehttps://en.wikipedia.org/wiki/Linear_least_squares\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Lorenzobattistela/linear-regression\"\u003ehttps://github.com/Lorenzobattistela/linear-regression\u003c/a\u003e\u003c/p\u003e\n","title":"Understanding Linear Regression","description":"Understanding linear regression principles and math.","date":"2024-05-16"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"understanding-linear-regression"},"buildId":"7XdtTWnKkLqV613zeyCgO","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>