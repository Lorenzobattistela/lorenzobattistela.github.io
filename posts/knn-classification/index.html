<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1.0" data-next-head=""/><meta name="author" content="Lorenzo Battistela" data-next-head=""/><link rel="icon" href="/favicon.ico" data-next-head=""/><title data-next-head=""></title><meta name="description" content="KNN classification algorithm" data-next-head=""/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/966ef0bc2ff51cf6.css" as="style"/><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&amp;family=IBM+Plex+Sans:wght@400;500;600;700&amp;display=swap" rel="stylesheet"/><link rel="stylesheet" href="/_next/static/css/966ef0bc2ff51cf6.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-afbe389e5387c555.js" defer=""></script><script src="/_next/static/chunks/framework-d7f578ab3069408c.js" defer=""></script><script src="/_next/static/chunks/main-a5703b490a653431.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d35945f6f2ae940c.js" defer=""></script><script src="/_next/static/chunks/230-8a10a6030a242aaa.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-9da8c7586f67ce5f.js" defer=""></script><script src="/_next/static/7XdtTWnKkLqV613zeyCgO/_buildManifest.js" defer=""></script><script src="/_next/static/7XdtTWnKkLqV613zeyCgO/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="container"><canvas class="background-canvas"></canvas><main class="post-content"><nav class="post-nav"><div class="post-nav-left"><a href="/">← Home</a></div><div class="post-nav-right"><div class="animation-controls"><button class="animation-toggle" aria-label="Disable background animation">Disable Animation</button><button class="animation-speed" aria-label="Current speed: Medium">Speed: <!-- -->Medium</button></div></div></nav><article><h1>What is K-Nearest-Neighbors classification?</h1><div class="post-meta"><time>2024-05-19</time></div><div class="markdown-content"><p>Starting your journey in Machine Learning and Data Science can be scary. You run into various terms that you don’t know and see things that you don’t understand at all. That’s how I felt when I first ran into algorithms like k-nearest-neighbors. But it is not that complicated! Let’s dive into this essential algorithm used for classification and regression. In this article, we will focus on the classification part of K-nearest-neighbors.</p>
<hr>
<h2>K-Nearest-Neighbors</h2>
<p>K-Nearest-Neighbors algorithm is getting old. It was first developed in 1951 <em>[1]</em>, by Evelyn Fix and Joseph Hodges, and it is widely used for classification and regression. In fact, this is a good starter algorithm to study. It is a non-parametric supervised learning. Non-parametric simply means that the number and nature of the parameters are flexible, and not fixed in advance.</p>
<p>Supervised learning refers to the learning process where the available data consists of labelled examples. For example, suppose I want to classify an email as <em>spam</em> or <em>ham</em>. The examples would consist in something like:</p>
<p>Text: “<em>Click here to win U$10k now, for free.”</em></p>
<p>Classification_: “spam”_</p>
<p>This category of learning aims to learn a function that maps feature vectors to labels based on example input-output pairs.</p>
<p>K-Nearest-Neighbors algorithm works based on an assumption: similar points can be found near one another. Think about a cartesian plane. What the assumption is saying is: two points that are close to each other in the plane are likely to be the same class.</p>
<p><img src="/images/knn-classification/1.webp" alt="KNN"></p>
<p>Source: <a href="https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning">https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning</a></p>
<p>Take a look in the image above. See that our examples are classified, they are either green (category A) or yellow (category B). KNN is not limited by 2 categories, but let’s work out this example. When a new data point comes to be classified, our algorithm calculates the distance between that point and the K nearest example points. Suppose that our algorithm used K=5 , and calculated that we have 3 nearest neighbors that are categorized as A and 2 categorized points as B. The algorithm would classify the new entry point as A, since the majority of our K nearest neighbors is A.</p>
<p>Ok, now we understand how does it work, but… How does it compute this distance? How can we find our nearest neighbors?</p>
<hr>
<h2>Computing Distance</h2>
<p>We can use many distance computing functions to use in KNN. In this article, I’ll cover two of them, that are widely known: Euclidean and Manhattan distance.</p>
<p><strong>Euclidean Distance</strong></p>
<p>Let’s start with Euclidean distance. We study this a lot in high school, but let’s recap.</p>
<p><img src="/images/knn-classification/2.webp" alt="Euclidean distance"></p>
<p>Suppose we have two points in the cartesian plane, being A = (2, 2) and B = (1, 5). Let’s work out the distance of them!</p>
<p><img src="/images/knn-classification/3.webp" alt=""></p>
<p>Computing this:</p>
<p><img src="/images/knn-classification/4.webp" alt="computation distance"></p>
<p>Now we know that the distance between the square root of 10. This is how our KNN algorithm know what points are closer to our new data point when we use Euclidean distance.</p>
<p><strong>Manhattan Distance</strong></p>
<p><img src="/images/knn-classification/5.webp" alt="manhattan distance"></p>
<p>This is the formula for our manhattan distance. It is named after the grid shape of the streets in Manhattan. Let’s work out the same example with A = (2, 2) and B = (1, 5). The computation will end up with:</p>
<p>| 1–2 | + | 5–2 | = 1 + 3 = 4</p>
<p>Our Mdist is 4 in this case, opposed to euclidean distance, square root of 10.</p>
<p><strong>Comparing Euclidean and Manhattan visually:</strong></p>
<p><img src="/images/knn-classification/6.webp" alt="comparing distances"></p>
<hr>
<h2>Parameter Selection</h2>
<p>In this algorithm, we have a parameter (that names it): K. But how to choose K? This is a question without a specific answer. The best choice of K depends upon the data. In general, if we have a large value of K, the effect of the noise in the classification is reduced (data and label noise are assumed deviations from the true dataset). When we have two classes, for example, it is nice to choose an odd K, to avoid ties. The number 5 is a good default number for K. However, a good K can be selected by various heuristic techniques.</p>
<p>We also have variations of KNN, like the Nearest Neighbor classifier and the Weighted nearest neighbor classifier. Taking care of the curse of the dimensionality is also really important. When we have high-dimensional data, dimension reduction is usually applied before applying KNN algorithm, avoiding the effects of the curse (in the KNN context, it would mean that the distance between two points is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector).</p>
<hr>
<h2>Result Validation</h2>
<p>A confusion (or matching) matrix is often used as a tool to validate the accuracy of KNN, but we also can apply more robust statistical methods. We will not cover this in this article.</p>
<hr>
<h2>Applications</h2>
<p>KNN is widely used, for example, in Recommendation engines. It can assign a particular user to a particular group, and give recommendations based on that group.</p>
<p>Another common use case is Data Preprocessing. We frequently run into datasets that have missing values. Our KNN algorithm can estimate these missing values, in a process that is known as Data Imputation.</p>
<p>We also have applications in finance, healthcare and pattern recognition.</p>
<hr>
<h2>Implementation</h2>
<p>Well, now we all know how the KNN algorithm works, so now let’s get our hands dirty and implement it. For learning purpose, we will use Scikit learn to achieve this.</p>
<p>I’ll leave my repository here: <a href="https://github.com/Lorenzobattistela/predict-iris">https://github.com/Lorenzobattistela/predict-iris</a></p>
<p>Drop a star if you like this article!</p>
<p>Let’s check out our <code>model.py</code> file.</p>
<p><img src="7.webp" alt="model.py"></p>
<p>Here we are just importing some functionalities we will use. I am using the iris dataset, train_test_split to split our training and testing data and our KNN classifier.</p>
<p><img src="8.webp" alt="model.py"></p>
<p>This piece of code refers to loading our dataset as a dataframe and returning it as a pandas DataFrame. ScikitLearn uses another dataset structure, but I decided to use pandas in this example.</p>
<p>In <code>get_test_train</code> , we split our data in training and testing, passing all columns that are not the target as X and target col as Y. Test size is 20% of our dataset, and random_state guarantees that when we load it again, the order will be the same (the random seed). Then we return our splitted dataset.</p>
<p><img src="9.webp" alt="model.py"></p>
<p>This function trains our classifier and return it. It is as simple as this. We simply fit our model with training data… and Done!</p>
<p><img src="10.webp" alt="model.py"></p>
<p>Let’s start with preprocess. Since we trained our model with labels, we need to pass what label each measure stands for when we are predicting. Prompt is a list with 4 floats (like [1.2, 5.3, 2.2, 4.1]). We process it and reshape it to fit our classifier with labels. Then, we can pass it to predict function.</p>
<p>The <code>predict</code> function will check for input, and simply predict our prompt. The result will be a number that stands for the class of our new data point. That’s why we need the get_iris_species function. Each number stands for one class. (0 for “iris setosa”, 1 for “iris versicolor” and 2 for “iris virginica”). Then we get what iris species our KNN algorithm spitted out, and we just created a KNN that predict iris dataset!</p>
<p>If you want to test the model with our training data, we also have a test function:</p>
<p><img src="11.webp" alt="model.py"></p>
<p>I simply calculated the percentage of correct predictions with our labels and returned the mean of it. There are other ways to measure error, but I chose the mean for example purposes.</p>
<hr>
<h2>Conclusion</h2>
<p>KNN is a widely known algorithm, that has a lot of applications. It is a good starter when we are learning data science and machine learning. We also have a lot of libraries that provide KNN classifiers, but I recommend you to try to build one from scratch. I did this and it was really helpful. Besides learning vector operations, train and test data splitting and performing calculations, it is really cool to have a model working that you built from nothing.</p>
<p>That’s all for today, if you liked this article, don’t forget to follow me!</p>
<hr>
<h2>References</h2>
<p>[1] Evelyn; Hodges, Joseph L. (1951).</p>
<p><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm?source=post_page-----a13cfc835818--------------------------------">k-nearest neighbors algorithm - Wikipedia</a></p>
<p><a href="https://www.ibm.com/topics/knn?source=post_page-----a13cfc835818--------------------------------">What is the k-nearest neighbors algorithm? | IBM</a></p>
<p><a href="https://towardsdatascience.com/data-noise-and-label-noise-in-machine-learning-98c8a3c8322e?source=post_page-----a13cfc835818--------------------------------">Data Noise and Label Noise in Machine Learning</a></p>
</div></article></main><footer><div class="footer-content"><p><a href="https://github.com/Lorenzobattistela" target="_blank" rel="noopener noreferrer">GitHub</a></p></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"knn-classification","contentHtml":"\u003cp\u003eStarting your journey in Machine Learning and Data Science can be scary. You run into various terms that you don’t know and see things that you don’t understand at all. That’s how I felt when I first ran into algorithms like k-nearest-neighbors. But it is not that complicated! Let’s dive into this essential algorithm used for classification and regression. In this article, we will focus on the classification part of K-nearest-neighbors.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eK-Nearest-Neighbors\u003c/h2\u003e\n\u003cp\u003eK-Nearest-Neighbors algorithm is getting old. It was first developed in 1951 \u003cem\u003e[1]\u003c/em\u003e, by Evelyn Fix and Joseph Hodges, and it is widely used for classification and regression. In fact, this is a good starter algorithm to study. It is a non-parametric supervised learning. Non-parametric simply means that the number and nature of the parameters are flexible, and not fixed in advance.\u003c/p\u003e\n\u003cp\u003eSupervised learning refers to the learning process where the available data consists of labelled examples. For example, suppose I want to classify an email as \u003cem\u003espam\u003c/em\u003e or \u003cem\u003eham\u003c/em\u003e. The examples would consist in something like:\u003c/p\u003e\n\u003cp\u003eText: “\u003cem\u003eClick here to win U$10k now, for free.”\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eClassification_: “spam”_\u003c/p\u003e\n\u003cp\u003eThis category of learning aims to learn a function that maps feature vectors to labels based on example input-output pairs.\u003c/p\u003e\n\u003cp\u003eK-Nearest-Neighbors algorithm works based on an assumption: similar points can be found near one another. Think about a cartesian plane. What the assumption is saying is: two points that are close to each other in the plane are likely to be the same class.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/knn-classification/1.webp\" alt=\"KNN\"\u003e\u003c/p\u003e\n\u003cp\u003eSource: \u003ca href=\"https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning\"\u003ehttps://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTake a look in the image above. See that our examples are classified, they are either green (category A) or yellow (category B). KNN is not limited by 2 categories, but let’s work out this example. When a new data point comes to be classified, our algorithm calculates the distance between that point and the K nearest example points. Suppose that our algorithm used K=5 , and calculated that we have 3 nearest neighbors that are categorized as A and 2 categorized points as B. The algorithm would classify the new entry point as A, since the majority of our K nearest neighbors is A.\u003c/p\u003e\n\u003cp\u003eOk, now we understand how does it work, but… How does it compute this distance? How can we find our nearest neighbors?\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eComputing Distance\u003c/h2\u003e\n\u003cp\u003eWe can use many distance computing functions to use in KNN. In this article, I’ll cover two of them, that are widely known: Euclidean and Manhattan distance.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEuclidean Distance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLet’s start with Euclidean distance. We study this a lot in high school, but let’s recap.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/knn-classification/2.webp\" alt=\"Euclidean distance\"\u003e\u003c/p\u003e\n\u003cp\u003eSuppose we have two points in the cartesian plane, being A = (2, 2) and B = (1, 5). Let’s work out the distance of them!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/knn-classification/3.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eComputing this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/knn-classification/4.webp\" alt=\"computation distance\"\u003e\u003c/p\u003e\n\u003cp\u003eNow we know that the distance between the square root of 10. This is how our KNN algorithm know what points are closer to our new data point when we use Euclidean distance.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eManhattan Distance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/knn-classification/5.webp\" alt=\"manhattan distance\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is the formula for our manhattan distance. It is named after the grid shape of the streets in Manhattan. Let’s work out the same example with A = (2, 2) and B = (1, 5). The computation will end up with:\u003c/p\u003e\n\u003cp\u003e| 1–2 | + | 5–2 | = 1 + 3 = 4\u003c/p\u003e\n\u003cp\u003eOur Mdist is 4 in this case, opposed to euclidean distance, square root of 10.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eComparing Euclidean and Manhattan visually:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/knn-classification/6.webp\" alt=\"comparing distances\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eParameter Selection\u003c/h2\u003e\n\u003cp\u003eIn this algorithm, we have a parameter (that names it): K. But how to choose K? This is a question without a specific answer. The best choice of K depends upon the data. In general, if we have a large value of K, the effect of the noise in the classification is reduced (data and label noise are assumed deviations from the true dataset). When we have two classes, for example, it is nice to choose an odd K, to avoid ties. The number 5 is a good default number for K. However, a good K can be selected by various heuristic techniques.\u003c/p\u003e\n\u003cp\u003eWe also have variations of KNN, like the Nearest Neighbor classifier and the Weighted nearest neighbor classifier. Taking care of the curse of the dimensionality is also really important. When we have high-dimensional data, dimension reduction is usually applied before applying KNN algorithm, avoiding the effects of the curse (in the KNN context, it would mean that the distance between two points is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector).\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eResult Validation\u003c/h2\u003e\n\u003cp\u003eA confusion (or matching) matrix is often used as a tool to validate the accuracy of KNN, but we also can apply more robust statistical methods. We will not cover this in this article.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eApplications\u003c/h2\u003e\n\u003cp\u003eKNN is widely used, for example, in Recommendation engines. It can assign a particular user to a particular group, and give recommendations based on that group.\u003c/p\u003e\n\u003cp\u003eAnother common use case is Data Preprocessing. We frequently run into datasets that have missing values. Our KNN algorithm can estimate these missing values, in a process that is known as Data Imputation.\u003c/p\u003e\n\u003cp\u003eWe also have applications in finance, healthcare and pattern recognition.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eImplementation\u003c/h2\u003e\n\u003cp\u003eWell, now we all know how the KNN algorithm works, so now let’s get our hands dirty and implement it. For learning purpose, we will use Scikit learn to achieve this.\u003c/p\u003e\n\u003cp\u003eI’ll leave my repository here: \u003ca href=\"https://github.com/Lorenzobattistela/predict-iris\"\u003ehttps://github.com/Lorenzobattistela/predict-iris\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDrop a star if you like this article!\u003c/p\u003e\n\u003cp\u003eLet’s check out our \u003ccode\u003emodel.py\u003c/code\u003e file.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"7.webp\" alt=\"model.py\"\u003e\u003c/p\u003e\n\u003cp\u003eHere we are just importing some functionalities we will use. I am using the iris dataset, train_test_split to split our training and testing data and our KNN classifier.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"8.webp\" alt=\"model.py\"\u003e\u003c/p\u003e\n\u003cp\u003eThis piece of code refers to loading our dataset as a dataframe and returning it as a pandas DataFrame. ScikitLearn uses another dataset structure, but I decided to use pandas in this example.\u003c/p\u003e\n\u003cp\u003eIn \u003ccode\u003eget_test_train\u003c/code\u003e , we split our data in training and testing, passing all columns that are not the target as X and target col as Y. Test size is 20% of our dataset, and random_state guarantees that when we load it again, the order will be the same (the random seed). Then we return our splitted dataset.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"9.webp\" alt=\"model.py\"\u003e\u003c/p\u003e\n\u003cp\u003eThis function trains our classifier and return it. It is as simple as this. We simply fit our model with training data… and Done!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"10.webp\" alt=\"model.py\"\u003e\u003c/p\u003e\n\u003cp\u003eLet’s start with preprocess. Since we trained our model with labels, we need to pass what label each measure stands for when we are predicting. Prompt is a list with 4 floats (like [1.2, 5.3, 2.2, 4.1]). We process it and reshape it to fit our classifier with labels. Then, we can pass it to predict function.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003epredict\u003c/code\u003e function will check for input, and simply predict our prompt. The result will be a number that stands for the class of our new data point. That’s why we need the get_iris_species function. Each number stands for one class. (0 for “iris setosa”, 1 for “iris versicolor” and 2 for “iris virginica”). Then we get what iris species our KNN algorithm spitted out, and we just created a KNN that predict iris dataset!\u003c/p\u003e\n\u003cp\u003eIf you want to test the model with our training data, we also have a test function:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"11.webp\" alt=\"model.py\"\u003e\u003c/p\u003e\n\u003cp\u003eI simply calculated the percentage of correct predictions with our labels and returned the mean of it. There are other ways to measure error, but I chose the mean for example purposes.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eKNN is a widely known algorithm, that has a lot of applications. It is a good starter when we are learning data science and machine learning. We also have a lot of libraries that provide KNN classifiers, but I recommend you to try to build one from scratch. I did this and it was really helpful. Besides learning vector operations, train and test data splitting and performing calculations, it is really cool to have a model working that you built from nothing.\u003c/p\u003e\n\u003cp\u003eThat’s all for today, if you liked this article, don’t forget to follow me!\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eReferences\u003c/h2\u003e\n\u003cp\u003e[1] Evelyn; Hodges, Joseph L. (1951).\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm?source=post_page-----a13cfc835818--------------------------------\"\u003ek-nearest neighbors algorithm - Wikipedia\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.ibm.com/topics/knn?source=post_page-----a13cfc835818--------------------------------\"\u003eWhat is the k-nearest neighbors algorithm? | IBM\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://towardsdatascience.com/data-noise-and-label-noise-in-machine-learning-98c8a3c8322e?source=post_page-----a13cfc835818--------------------------------\"\u003eData Noise and Label Noise in Machine Learning\u003c/a\u003e\u003c/p\u003e\n","title":"What is K-Nearest-Neighbors classification?","description":"KNN classification algorithm","date":"2024-05-19"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"knn-classification"},"buildId":"7XdtTWnKkLqV613zeyCgO","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>