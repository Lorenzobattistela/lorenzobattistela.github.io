<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1.0" data-next-head=""/><meta name="author" content="Lorenzo Battistela" data-next-head=""/><link rel="icon" href="/favicon.ico" data-next-head=""/><title data-next-head=""></title><meta name="description" content="A guide to understanding different types of regression algorithms" data-next-head=""/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/966ef0bc2ff51cf6.css" as="style"/><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&amp;family=IBM+Plex+Sans:wght@400;500;600;700&amp;display=swap" rel="stylesheet"/><link rel="stylesheet" href="/_next/static/css/966ef0bc2ff51cf6.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-afbe389e5387c555.js" defer=""></script><script src="/_next/static/chunks/framework-d7f578ab3069408c.js" defer=""></script><script src="/_next/static/chunks/main-a5703b490a653431.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d35945f6f2ae940c.js" defer=""></script><script src="/_next/static/chunks/230-8a10a6030a242aaa.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-9da8c7586f67ce5f.js" defer=""></script><script src="/_next/static/ydYcoKj9y-mj7XR-0MT3a/_buildManifest.js" defer=""></script><script src="/_next/static/ydYcoKj9y-mj7XR-0MT3a/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="container"><canvas class="background-canvas"></canvas><main class="post-content"><nav class="post-nav"><div class="post-nav-left"><a href="/">← Home</a></div><div class="post-nav-right"><div class="animation-controls"><button class="animation-toggle" aria-label="Disable background animation">Disable Animation</button><button class="animation-speed" aria-label="Current speed: Medium">Speed: <!-- -->Medium</button></div></div></nav><article><h1>Understanding Different types of Regression</h1><div class="post-meta"><time>2024-05-20</time></div><div class="markdown-content"><p>Regression algorithms are widely used in the world of Data Science and Machine Learning. In my previous articles, I explained Linear and Multiple Linear regression. But that’s not all. To close the regression topic, today I’ll cover different types of regression algorithms, what they do and how to use them. Let’s dive in!</p>
<p>As explained in my previous articles, a simple linear regression model makes a prediction by a ponderated sum of input features plus a constant called polarization term (or linear coefficient). Training a model means configure your parameters to be the best fit on the training set. For this to be done, we have to measure how good or bad the model is adapting to data. To train a linear regression model, we are finding the value that minimizes MSE (medium square error). The function can be described as</p>
<p><img src="/images/different-regression-algorithms/1.webp" alt="function"></p>
<p>To find the value of theta that minimizes the cost function, we can use the least squares method:</p>
<p><img src="/images/different-regression-algorithms/2.webp" alt="lsm"></p>
<p>Where theta is the vector that minimizes the cost function and y is the vector of target values from y1 ym. Let’s look closely to an example in python:</p>
<pre><code class="language-python">import numpy as np  
import matplotlib.pyplot as plt  
  
X = 2\* np.random.rand(100,1)  
y = 4 + 3 \* X + np.random.randn(100,1)  
  
plt.scatter(X, y)  
plt.xlabel(&#39;x&#39;)  
plt.ylabel(&#39;y&#39;)  
plt.show()
</code></pre>
<p><img src="/images/different-regression-algorithms/3.webp" alt="generated data points"></p>
<p>First, we generate our data points and plot them to see what they look like. The function we used to generate our data points is _y = 4 + 3x + gaussian noise ._Now, we’re going to apply the formula mentioned above to find the best theta value for our data points:</p>
<pre><code class="language-python">X_b = np.c_[np.ones((100,1)), X] # adds x0 = 1 to each instance  
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)  
theta_best  
  
# array([[4.00634639], [2.94157425]])
</code></pre>
<p>We can see that we did not get 4 and 3, which should be the result, but it was close enough (we didn’t get it exactly because of gaussian noise).</p>
<p>Let’s plot some prediction points as a line and understand how our model fit training data:</p>
<p><img src="/images/different-regression-algorithms/4.webp" alt="data predictions"></p>
<p>We can see that our line fits data points pretty well for a simple linear model. The computational complexity for this is heavy (typically O(n²) to O(n³) for inverting a matrix). The predictions are fast O(n).</p>
<hr>
<h2>Polynomial Regression</h2>
<p>We can see that our data points were really simple in the previous model. But what if our data does not fit a simple straight line? Well, we can use a linear model to fit non-linear data. A simple way to do this is adding powers of each feature as new features, and train a linear model in this extended set of features. This technique is called Polynomial Regression.</p>
<p>We can do this using PolynomialFeatures from scikit learn library. For example:</p>
<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures  
  
m = 100  
X = 6 * np.random.rand(m, 1) - 3  
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)  
  
poly_features = PolynomialFeatures(degree=2, include_bias=false)  
X_poly = poly_features.fit_transform(X)  
X_poly[0]  
  
# array([-0.752734872, 0.5666234])
</code></pre>
<p>Using PolynomialFeatures allow us to add the square of each feature to the training model (in this case we have just one feature). Now, X_poly has the original feature X plus X squared. Note that when we have many features, Polynomial Regression is capable of finding relationships between them (something that a Linear Regression cannot do). This is possible because PolynomialFeatures also adds all combinations of features to the degree provided. This means if we had <em>a</em> and <em>b</em> with degree=3, we would have:</p>
<p><em>a², a³, b², b³, ab, a²b, ab²</em></p>
<hr>
<h2>Ridge Regression</h2>
<p>Ridge regression is a regularized version of linear regression. In general, for linear models, regularization is done by restricting model weights. In Ridge, a regularization term is added to the cost function. The hyperparameter used controls how much you want to regularize the model. Closer to 1, weights will end up close to 0 and the line close to the average. The polarization term is not regularized. An easy way to execute a ridge regression is:</p>
<pre><code class="language-python">from sklearn.linear_model import Ridge  
  
# assuming you already have X and y   
  
ridge_reg = Ridge(alpha=1, solver=&quot;cholesky&quot;)  
ridge_reg.fit(X, y)  
ridge_reg.predict([[1.5]]) # predicting some value
</code></pre>
<p><img src="/images/different-regression-algorithms/5.webp" alt="ridge regression equation"></p>
<hr>
<h2>Lasso Regression</h2>
<p>It is also a regularized version of linear regression, and it adds a regularization term to the cost functions, but it uses the norm <em>l1</em> to the weight instead of the half of the square of the norm <em>l2</em>. Lasso regression tends to eliminate completely the weights of features with less importance (adjust them to 0). It automatically executes feature selection and displays a sparse model (few feature weights != 0).</p>
<pre><code class="language-py">from sklearn.linear_model import Lasso  
lasso_reg = Lasso(alpha=0.1)  
lasso_reg.fit(X, y)  
lasso_reg.predict([[1.5]])  
  
# array([8.28714555])
</code></pre>
<p><img src="/images/different-regression-algorithms/6.webp" alt="lasso regression equation"></p>
<hr>
<h2>Elastic Net</h2>
<p>It is a mid term between Ridge and Lasso. The regularization term is simply a mix of the regularization terms of Ridge and Lasso, and you can control the ‘mixing rate’ r. When r=0, Elastic Net is the same thing as Ridge, and r=1 it is equal to Lasso.</p>
<pre><code class="language-py">from sklearn.linear_model import ElasticNet  
  
# l1 ratio l1_ratio is mixing tax  
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  
elastic_net.fit(X,y)  
elastic_net.predict([[1.5]])  
  
# array([8.18453219])
</code></pre>
<p><img src="/images/different-regression-algorithms/7.webp" alt="elastic net equation"></p>
<p>To choose between regression, we need to take some things in account. Generally speaking, it is not good to use linear regression without any regularization. Ridge is a good pattern, but if you can tell that some features are useless, you can choose between Lasso and Elastic Net, since they tend to reduce useless features to 0. In general, Elastic Net is preferable to Lasso, that can behave wrongly when feature number is higher than training instances number or highly correlated features.</p>
<hr>
<h2>Logistic Regression</h2>
<p>Commonly used to estimate the probability of an instance belong to a determined category. It calculates the ponderated sum of input features and it generates the logistic of this result.</p>
<p><img src="/images/different-regression-algorithms/8.webp" alt="logistic regression equation"></p>
<p>The cost relation with all training set is simply the average cost related to all training instances, and it is written in an expression called log loss:</p>
<p><img src="/images/different-regression-algorithms/9.webp" alt="logistic loss"></p>
<hr>
<h2>Softmax Regression</h2>
<p>Logistic regression can be generalized for multiple classes directly without the need to train and combine many binary classifiers. This is called Softmax Regression, or Multinomial Logistic Regression.</p>
<p>Given an instance x, softmax model will calculated a punctuation sk(x) to each class k, and then estimate the probability for each class applying the softmax function (normalized exponential) to the punctuations. Once calculated, you can estimate the probability of Phat k of an instance to belong to the class k. It calculates the exponencial of each punctuation and normalizes it by dividing it by the sum of all exponentials.</p>
<hr>
<h2>Conclusion</h2>
<p>Regression algorithms are used very often to study and to predict linear (and even non linear) data. Linear regression is the most common, but it is important to know that different cases require different algorithms. That said, it is important to know how other algorithms work, so you can pick the right one when needed! Thanks for reading it, see you next week!</p>
<p>— Lorenzo</p>
<h2>References:</h2>
<p>Mãos à Obra: Aprendizado de Máquina com Scikit-Learn &amp; TensorFlow — Aurélien Geron</p>
<p><a href="https://en.wikipedia.org/wiki/Linear_regression?source=post_page-----19f4442cc233--------------------------------">Linear regression - Wikipedia</a></p>
<p><a href="https://en.wikipedia.org/wiki/Logistic_regression?source=post_page-----19f4442cc233--------------------------------">Logistic regression - Wikipedia</a></p>
<p><a href="https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3">https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3</a></p>
</div></article></main><footer><div class="footer-content"><p><a href="https://github.com/Lorenzobattistela" target="_blank" rel="noopener noreferrer">GitHub</a></p></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"different-regression-algorithms","contentHtml":"\u003cp\u003eRegression algorithms are widely used in the world of Data Science and Machine Learning. In my previous articles, I explained Linear and Multiple Linear regression. But that’s not all. To close the regression topic, today I’ll cover different types of regression algorithms, what they do and how to use them. Let’s dive in!\u003c/p\u003e\n\u003cp\u003eAs explained in my previous articles, a simple linear regression model makes a prediction by a ponderated sum of input features plus a constant called polarization term (or linear coefficient). Training a model means configure your parameters to be the best fit on the training set. For this to be done, we have to measure how good or bad the model is adapting to data. To train a linear regression model, we are finding the value that minimizes MSE (medium square error). The function can be described as\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/different-regression-algorithms/1.webp\" alt=\"function\"\u003e\u003c/p\u003e\n\u003cp\u003eTo find the value of theta that minimizes the cost function, we can use the least squares method:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/different-regression-algorithms/2.webp\" alt=\"lsm\"\u003e\u003c/p\u003e\n\u003cp\u003eWhere theta is the vector that minimizes the cost function and y is the vector of target values from y1 ym. Let’s look closely to an example in python:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport numpy as np  \nimport matplotlib.pyplot as plt  \n  \nX = 2\\* np.random.rand(100,1)  \ny = 4 + 3 \\* X + np.random.randn(100,1)  \n  \nplt.scatter(X, y)  \nplt.xlabel(\u0026#39;x\u0026#39;)  \nplt.ylabel(\u0026#39;y\u0026#39;)  \nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/different-regression-algorithms/3.webp\" alt=\"generated data points\"\u003e\u003c/p\u003e\n\u003cp\u003eFirst, we generate our data points and plot them to see what they look like. The function we used to generate our data points is _y = 4 + 3x + gaussian noise ._Now, we’re going to apply the formula mentioned above to find the best theta value for our data points:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eX_b = np.c_[np.ones((100,1)), X] # adds x0 = 1 to each instance  \ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)  \ntheta_best  \n  \n# array([[4.00634639], [2.94157425]])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe can see that we did not get 4 and 3, which should be the result, but it was close enough (we didn’t get it exactly because of gaussian noise).\u003c/p\u003e\n\u003cp\u003eLet’s plot some prediction points as a line and understand how our model fit training data:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/different-regression-algorithms/4.webp\" alt=\"data predictions\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can see that our line fits data points pretty well for a simple linear model. The computational complexity for this is heavy (typically O(n²) to O(n³) for inverting a matrix). The predictions are fast O(n).\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003ePolynomial Regression\u003c/h2\u003e\n\u003cp\u003eWe can see that our data points were really simple in the previous model. But what if our data does not fit a simple straight line? Well, we can use a linear model to fit non-linear data. A simple way to do this is adding powers of each feature as new features, and train a linear model in this extended set of features. This technique is called Polynomial Regression.\u003c/p\u003e\n\u003cp\u003eWe can do this using PolynomialFeatures from scikit learn library. For example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom sklearn.preprocessing import PolynomialFeatures  \n  \nm = 100  \nX = 6 * np.random.rand(m, 1) - 3  \ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)  \n  \npoly_features = PolynomialFeatures(degree=2, include_bias=false)  \nX_poly = poly_features.fit_transform(X)  \nX_poly[0]  \n  \n# array([-0.752734872, 0.5666234])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUsing PolynomialFeatures allow us to add the square of each feature to the training model (in this case we have just one feature). Now, X_poly has the original feature X plus X squared. Note that when we have many features, Polynomial Regression is capable of finding relationships between them (something that a Linear Regression cannot do). This is possible because PolynomialFeatures also adds all combinations of features to the degree provided. This means if we had \u003cem\u003ea\u003c/em\u003e and \u003cem\u003eb\u003c/em\u003e with degree=3, we would have:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003ea², a³, b², b³, ab, a²b, ab²\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eRidge Regression\u003c/h2\u003e\n\u003cp\u003eRidge regression is a regularized version of linear regression. In general, for linear models, regularization is done by restricting model weights. In Ridge, a regularization term is added to the cost function. The hyperparameter used controls how much you want to regularize the model. Closer to 1, weights will end up close to 0 and the line close to the average. The polarization term is not regularized. An easy way to execute a ridge regression is:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom sklearn.linear_model import Ridge  \n  \n# assuming you already have X and y   \n  \nridge_reg = Ridge(alpha=1, solver=\u0026quot;cholesky\u0026quot;)  \nridge_reg.fit(X, y)  \nridge_reg.predict([[1.5]]) # predicting some value\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/different-regression-algorithms/5.webp\" alt=\"ridge regression equation\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eLasso Regression\u003c/h2\u003e\n\u003cp\u003eIt is also a regularized version of linear regression, and it adds a regularization term to the cost functions, but it uses the norm \u003cem\u003el1\u003c/em\u003e to the weight instead of the half of the square of the norm \u003cem\u003el2\u003c/em\u003e. Lasso regression tends to eliminate completely the weights of features with less importance (adjust them to 0). It automatically executes feature selection and displays a sparse model (few feature weights != 0).\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003efrom sklearn.linear_model import Lasso  \nlasso_reg = Lasso(alpha=0.1)  \nlasso_reg.fit(X, y)  \nlasso_reg.predict([[1.5]])  \n  \n# array([8.28714555])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/different-regression-algorithms/6.webp\" alt=\"lasso regression equation\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eElastic Net\u003c/h2\u003e\n\u003cp\u003eIt is a mid term between Ridge and Lasso. The regularization term is simply a mix of the regularization terms of Ridge and Lasso, and you can control the ‘mixing rate’ r. When r=0, Elastic Net is the same thing as Ridge, and r=1 it is equal to Lasso.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003efrom sklearn.linear_model import ElasticNet  \n  \n# l1 ratio l1_ratio is mixing tax  \nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  \nelastic_net.fit(X,y)  \nelastic_net.predict([[1.5]])  \n  \n# array([8.18453219])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/different-regression-algorithms/7.webp\" alt=\"elastic net equation\"\u003e\u003c/p\u003e\n\u003cp\u003eTo choose between regression, we need to take some things in account. Generally speaking, it is not good to use linear regression without any regularization. Ridge is a good pattern, but if you can tell that some features are useless, you can choose between Lasso and Elastic Net, since they tend to reduce useless features to 0. In general, Elastic Net is preferable to Lasso, that can behave wrongly when feature number is higher than training instances number or highly correlated features.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eLogistic Regression\u003c/h2\u003e\n\u003cp\u003eCommonly used to estimate the probability of an instance belong to a determined category. It calculates the ponderated sum of input features and it generates the logistic of this result.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/different-regression-algorithms/8.webp\" alt=\"logistic regression equation\"\u003e\u003c/p\u003e\n\u003cp\u003eThe cost relation with all training set is simply the average cost related to all training instances, and it is written in an expression called log loss:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/different-regression-algorithms/9.webp\" alt=\"logistic loss\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eSoftmax Regression\u003c/h2\u003e\n\u003cp\u003eLogistic regression can be generalized for multiple classes directly without the need to train and combine many binary classifiers. This is called Softmax Regression, or Multinomial Logistic Regression.\u003c/p\u003e\n\u003cp\u003eGiven an instance x, softmax model will calculated a punctuation sk(x) to each class k, and then estimate the probability for each class applying the softmax function (normalized exponential) to the punctuations. Once calculated, you can estimate the probability of Phat k of an instance to belong to the class k. It calculates the exponencial of each punctuation and normalizes it by dividing it by the sum of all exponentials.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eRegression algorithms are used very often to study and to predict linear (and even non linear) data. Linear regression is the most common, but it is important to know that different cases require different algorithms. That said, it is important to know how other algorithms work, so you can pick the right one when needed! Thanks for reading it, see you next week!\u003c/p\u003e\n\u003cp\u003e— Lorenzo\u003c/p\u003e\n\u003ch2\u003eReferences:\u003c/h2\u003e\n\u003cp\u003eMãos à Obra: Aprendizado de Máquina com Scikit-Learn \u0026amp; TensorFlow — Aurélien Geron\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Linear_regression?source=post_page-----19f4442cc233--------------------------------\"\u003eLinear regression - Wikipedia\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Logistic_regression?source=post_page-----19f4442cc233--------------------------------\"\u003eLogistic regression - Wikipedia\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3\"\u003ehttps://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3\u003c/a\u003e\u003c/p\u003e\n","title":"Understanding Different types of Regression","description":"A guide to understanding different types of regression algorithms","date":"2024-05-20"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"different-regression-algorithms"},"buildId":"ydYcoKj9y-mj7XR-0MT3a","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>