<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1.0" data-next-head=""/><meta name="author" content="Lorenzo Battistela" data-next-head=""/><link rel="icon" href="/favicon.ico" data-next-head=""/><title data-next-head=""></title><meta name="description" content="Multiple linear regression. What it is and its uses." data-next-head=""/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/966ef0bc2ff51cf6.css" as="style"/><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&amp;family=IBM+Plex+Sans:wght@400;500;600;700&amp;display=swap" rel="stylesheet"/><link rel="stylesheet" href="/_next/static/css/966ef0bc2ff51cf6.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-afbe389e5387c555.js" defer=""></script><script src="/_next/static/chunks/framework-d7f578ab3069408c.js" defer=""></script><script src="/_next/static/chunks/main-a5703b490a653431.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d35945f6f2ae940c.js" defer=""></script><script src="/_next/static/chunks/230-8a10a6030a242aaa.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-9da8c7586f67ce5f.js" defer=""></script><script src="/_next/static/7XdtTWnKkLqV613zeyCgO/_buildManifest.js" defer=""></script><script src="/_next/static/7XdtTWnKkLqV613zeyCgO/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="container"><canvas class="background-canvas"></canvas><main class="post-content"><nav class="post-nav"><div class="post-nav-left"><a href="/">‚Üê Home</a></div><div class="post-nav-right"><div class="animation-controls"><button class="animation-toggle" aria-label="Disable background animation">Disable Animation</button><button class="animation-speed" aria-label="Current speed: Medium">Speed: <!-- -->Medium</button></div></div></nav><article><h1>Understanding multiple linear regression</h1><div class="post-meta"><time>2024-05-17</time></div><div class="markdown-content"><p>On my previous article about linear regression, I defined it as a linear approach for modelling the relationship between a scalar response and one or more explanatory variables, and focused and models with one explanatory variable. Today, we are going to talk about multiple linear regression, which is still linear regression but with two or more predictors.</p>
<p>Suppose you are the manager of a company, and you want to understand how the marketing of your products affects the number of sold products. Lets say you have Instagram divulgation, Facebook ads and Google ads. They can affect product selling in different ways, and if we apply simple linear regression here fixing Facebook ads, for example, the result would be inaccurate because we would not consider other influences.</p>
<p>That said, we extend our simple linear regression model to support more than on explanatory variable. Mathematically speaking, we have:</p>
<p><img src="equation1.webp" alt="equation1"></p>
<p>Where <em>y</em> is the output value, the <em>X</em> terms are input variables (explanatory), and each predictor (x) has a slope coefficient (B).</p>
<p>Taking it to our example, we can say that:</p>
<pre><code>Products sold = B0 + B1 \* Instagram + B2 \* Facebook + B3 \* Google
</code></pre>
<p>The model would find the slope constants minimizing error function and fitting the best line. Again, we use Residual Sum of Squares (OLS):</p>
<p><img src="equation2.webp" alt="equation2"></p>
<p>Suppose that we have a real dataset on the example above, and that we ran our Linear Regression model:</p>
<pre><code>Products sold = 1.92123 + 0.15021 \* Instagram + 0.05121 \* Facebook - 0.00123 \* Google
</code></pre>
<p>This means that if we fix the budget for Facebook and Google, increasing Instagram budget on advertisement by $1000 will increase our product sales by around 150 units (0.150 * 1000). We also see that Facebook is not affecting sold products.</p>
<p>But if we run Simple linear regression on Facebook, it would probably output a higher value (suppose 0.066), which would influence sales. Lets understand collinearity.</p>
<p>Collinearity is defined as a linear relationship between two explanatory variables. This means that, if we have perfect collinearity (C(y1, y2) = 1), the slope (B) of this variables is the same for all observations, which means they are perfectly linearly related.</p>
<p>Practically speaking, this means that a variable can influence another, they can be related linearly. For our example, suppose we measured the correlation between facebook and google, and the result was 0.40</p>
<p>This indicates a fair relationship between the two measures, and we can infer that when we facebook budget increase sold products, there is a tendency to google to increase it as well. This is why a simple linear regression will show impact on sales, because if we ran SLR, we would have only one value but the impact of both would be counted.</p>
<p>We also have to think is wether or not our predictors are useful in predicting the output. This is where forming Hypothesis come. The model we were discussing had numbers that were able to establish a close enough linear relationship, but this can be an accident and we did not prove the credibility of that relationship. So, we do a Hypothesis Test.</p>
<p>Lets start by forming a Null Hypothesis and an Alternative one. The Null hypothesis is that all coeficients (B) are equal to 0. The alternative is that at least one of the coefficients is not zero.</p>
<p>For this, we will use F-statistic (known as fixation indices). I will not cover it in this article, but if the value of it is equal to or very close to 1, results are in favor of Null Hypothesis. Note that F-statistic is not suitable when number of predictors is large or if you have more predictors than data samples.</p>
<p>So, we can try every combination of variables in our case (since we have only 3), and perform feature selection using different approaches, such as Forward or Backward selection.</p>
<p>Forward selection: start with a model without any predictor, but with the interceptor (B0). Perform linear regression for each predictor to find best performer (low residual sum of squares). Then, we add another variable and check vor the best 2 variable combination for lower RSS. We perform this step untill a stopping rule is statisfied (maybe 4 variable combination or other condition, is up to you).</p>
<p>Backward selection: start with all variables in the model and remove the least statistically significant . This is repeated untill a stopping rule is reached or when the model score cannot be improved.</p>
<h1>Linear Regression Assumptions</h1>
<p>When performing linear regression, we assume some thing about our data.</p>
<ul>
<li>The response variable is continuous and the explanatory variables are either continuous or binary</li>
<li>The relationship between outcome and explanatory variables is linear</li>
<li>Residuals are homoschedastic</li>
<li>Residuals are normally distributed</li>
<li>There is no more than limited multicollinearity</li>
<li>There are no other external variables</li>
<li>Independent errors</li>
<li>Independent Observations</li>
</ul>
<p>If those assumptions are proven to not be true, this does not mean that the model is unusable, but there will be limitations.</p>
<h1>Extreme Values</h1>
<p>Sometimes, when analysing regression, it is useful to remove outliers from data before refining model. It will have huge effects on coefficients (B), because the LSS method minimises squared error terms, and this can move the line if we have extreme values. When an outlier has this influence, it is described as having leverage on our regression line.</p>
<p>We can measure this using Cooks distance, which is a measure of the change in predicted values if an observation x is removed. Any value with a distance larger than three times the mean of Cooks distance might be an outlier.</p>
<h1>Conclusion</h1>
<p>Thanks for reading this! Hope it helps.</p>
<p>This article was written with the unique purpose of studying and passing on knowledge. I have used other articles to study and will reference them in References section.</p>
<h1>References</h1>
<p><a href="https://towardsdatascience.com/multiple-linear-regression-8cf3bee21d8b?source=post_page-----ec4a04d7b4ed--------------------------------">Multiple Linear Regression</a></p>
<p><a href="https://blog.minitab.com/en/adventures-in-statistics-2/what-is-the-difference-between-linear-and-nonlinear-equations-in-regression-analysis?source=post_page-----ec4a04d7b4ed--------------------------------">What is the difference between linear and nonlinear equations</a></p>
<p><a href="https://www.scribbr.com/statistics/multiple-linear-regression/?source=post_page-----ec4a04d7b4ed--------------------------------">Multiple Linear Regression | A Quick Guide (Examples)</a></p>
</div></article></main><footer><div class="footer-content"><p><a href="https://github.com/Lorenzobattistela" target="_blank" rel="noopener noreferrer">GitHub</a></p></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"understanding-multiple-linear-regression","contentHtml":"\u003cp\u003eOn my previous article about linear regression, I defined it as a linear approach for modelling the relationship between a scalar response and one or more explanatory variables, and focused and models with one explanatory variable. Today, we are going to talk about multiple linear regression, which is still linear regression but with two or more predictors.\u003c/p\u003e\n\u003cp\u003eSuppose you are the manager of a company, and you want to understand how the marketing of your products affects the number of sold products. Lets say you have Instagram divulgation, Facebook ads and Google ads. They can affect product selling in different ways, and if we apply simple linear regression here fixing Facebook ads, for example, the result would be inaccurate because we would not consider other influences.\u003c/p\u003e\n\u003cp\u003eThat said, we extend our simple linear regression model to support more than on explanatory variable. Mathematically speaking, we have:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"equation1.webp\" alt=\"equation1\"\u003e\u003c/p\u003e\n\u003cp\u003eWhere \u003cem\u003ey\u003c/em\u003e is the output value, the \u003cem\u003eX\u003c/em\u003e terms are input variables (explanatory), and each predictor (x) has a slope coefficient (B).\u003c/p\u003e\n\u003cp\u003eTaking it to our example, we can say that:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eProducts sold = B0 + B1 \\* Instagram + B2 \\* Facebook + B3 \\* Google\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe model would find the slope constants minimizing error function and fitting the best line. Again, we use Residual Sum of Squares (OLS):\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"equation2.webp\" alt=\"equation2\"\u003e\u003c/p\u003e\n\u003cp\u003eSuppose that we have a real dataset on the example above, and that we ran our Linear Regression model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eProducts sold = 1.92123 + 0.15021 \\* Instagram + 0.05121 \\* Facebook - 0.00123 \\* Google\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis means that if we fix the budget for Facebook and Google, increasing Instagram budget on advertisement by $1000 will increase our product sales by around 150 units (0.150 * 1000). We also see that Facebook is not affecting sold products.\u003c/p\u003e\n\u003cp\u003eBut if we run Simple linear regression on Facebook, it would probably output a higher value (suppose 0.066), which would influence sales. Lets understand collinearity.\u003c/p\u003e\n\u003cp\u003eCollinearity is defined as a linear relationship between two explanatory variables. This means that, if we have perfect collinearity (C(y1, y2) = 1), the slope (B) of this variables is the same for all observations, which means they are perfectly linearly related.\u003c/p\u003e\n\u003cp\u003ePractically speaking, this means that a variable can influence another, they can be related linearly. For our example, suppose we measured the correlation between facebook and google, and the result was 0.40\u003c/p\u003e\n\u003cp\u003eThis indicates a fair relationship between the two measures, and we can infer that when we facebook budget increase sold products, there is a tendency to google to increase it as well. This is why a simple linear regression will show impact on sales, because if we ran SLR, we would have only one value but the impact of both would be counted.\u003c/p\u003e\n\u003cp\u003eWe also have to think is wether or not our predictors are useful in predicting the output. This is where forming Hypothesis come. The model we were discussing had numbers that were able to establish a close enough linear relationship, but this can be an accident and we did not prove the credibility of that relationship. So, we do a Hypothesis Test.\u003c/p\u003e\n\u003cp\u003eLets start by forming a Null Hypothesis and an Alternative one. The Null hypothesis is that all coeficients (B) are equal to 0. The alternative is that at least one of the coefficients is not zero.\u003c/p\u003e\n\u003cp\u003eFor this, we will use F-statistic (known as fixation indices). I will not cover it in this article, but if the value of it is equal to or very close to 1, results are in favor of Null Hypothesis. Note that F-statistic is not suitable when number of predictors is large or if you have more predictors than data samples.\u003c/p\u003e\n\u003cp\u003eSo, we can try every combination of variables in our case (since we have only 3), and perform feature selection using different approaches, such as Forward or Backward selection.\u003c/p\u003e\n\u003cp\u003eForward selection: start with a model without any predictor, but with the interceptor (B0). Perform linear regression for each predictor to find best performer (low residual sum of squares). Then, we add another variable and check vor the best 2 variable combination for lower RSS. We perform this step untill a stopping rule is statisfied (maybe 4 variable combination or other condition, is up to you).\u003c/p\u003e\n\u003cp\u003eBackward selection: start with all variables in the model and remove the least statistically significant . This is repeated untill a stopping rule is reached or when the model score cannot be improved.\u003c/p\u003e\n\u003ch1\u003eLinear Regression Assumptions\u003c/h1\u003e\n\u003cp\u003eWhen performing linear regression, we assume some thing about our data.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe response variable is continuous and the explanatory variables are either continuous or binary\u003c/li\u003e\n\u003cli\u003eThe relationship between outcome and explanatory variables is linear\u003c/li\u003e\n\u003cli\u003eResiduals are homoschedastic\u003c/li\u003e\n\u003cli\u003eResiduals are normally distributed\u003c/li\u003e\n\u003cli\u003eThere is no more than limited multicollinearity\u003c/li\u003e\n\u003cli\u003eThere are no other external variables\u003c/li\u003e\n\u003cli\u003eIndependent errors\u003c/li\u003e\n\u003cli\u003eIndependent Observations\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf those assumptions are proven to not be true, this does not mean that the model is unusable, but there will be limitations.\u003c/p\u003e\n\u003ch1\u003eExtreme Values\u003c/h1\u003e\n\u003cp\u003eSometimes, when analysing regression, it is useful to remove outliers from data before refining model. It will have huge effects on coefficients (B), because the LSS method minimises squared error terms, and this can move the line if we have extreme values. When an outlier has this influence, it is described as having leverage on our regression line.\u003c/p\u003e\n\u003cp\u003eWe can measure this using Cooks distance, which is a measure of the change in predicted values if an observation x is removed. Any value with a distance larger than three times the mean of Cooks distance might be an outlier.\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eThanks for reading this! Hope it helps.\u003c/p\u003e\n\u003cp\u003eThis article was written with the unique purpose of studying and passing on knowledge. I have used other articles to study and will reference them in References section.\u003c/p\u003e\n\u003ch1\u003eReferences\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://towardsdatascience.com/multiple-linear-regression-8cf3bee21d8b?source=post_page-----ec4a04d7b4ed--------------------------------\"\u003eMultiple Linear Regression\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://blog.minitab.com/en/adventures-in-statistics-2/what-is-the-difference-between-linear-and-nonlinear-equations-in-regression-analysis?source=post_page-----ec4a04d7b4ed--------------------------------\"\u003eWhat is the difference between linear and nonlinear equations\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.scribbr.com/statistics/multiple-linear-regression/?source=post_page-----ec4a04d7b4ed--------------------------------\"\u003eMultiple Linear Regression | A Quick Guide (Examples)\u003c/a\u003e\u003c/p\u003e\n","title":"Understanding multiple linear regression","description":"Multiple linear regression. What it is and its uses.","date":"2024-05-17"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"understanding-multiple-linear-regression"},"buildId":"7XdtTWnKkLqV613zeyCgO","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>