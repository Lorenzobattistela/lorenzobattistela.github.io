{"pageProps":{"postData":{"id":"different-regression-algorithms","contentHtml":"<p>Regression algorithms are widely used in the world of Data Science and Machine Learning. In my previous articles, I explained Linear and Multiple Linear regression. But that’s not all. To close the regression topic, today I’ll cover different types of regression algorithms, what they do and how to use them. Let’s dive in!</p>\n<p>As explained in my previous articles, a simple linear regression model makes a prediction by a ponderated sum of input features plus a constant called polarization term (or linear coefficient). Training a model means configure your parameters to be the best fit on the training set. For this to be done, we have to measure how good or bad the model is adapting to data. To train a linear regression model, we are finding the value that minimizes MSE (medium square error). The function can be described as</p>\n<p><img src=\"/images/different-regression-algorithms/1.webp\" alt=\"function\"></p>\n<p>To find the value of theta that minimizes the cost function, we can use the least squares method:</p>\n<p><img src=\"/images/different-regression-algorithms/2.webp\" alt=\"lsm\"></p>\n<p>Where theta is the vector that minimizes the cost function and y is the vector of target values from y1 ym. Let’s look closely to an example in python:</p>\n<pre><code class=\"language-python\">import numpy as np  \nimport matplotlib.pyplot as plt  \n  \nX = 2\\* np.random.rand(100,1)  \ny = 4 + 3 \\* X + np.random.randn(100,1)  \n  \nplt.scatter(X, y)  \nplt.xlabel(&#39;x&#39;)  \nplt.ylabel(&#39;y&#39;)  \nplt.show()\n</code></pre>\n<p><img src=\"/images/different-regression-algorithms/3.webp\" alt=\"generated data points\"></p>\n<p>First, we generate our data points and plot them to see what they look like. The function we used to generate our data points is _y = 4 + 3x + gaussian noise ._Now, we’re going to apply the formula mentioned above to find the best theta value for our data points:</p>\n<pre><code class=\"language-python\">X_b = np.c_[np.ones((100,1)), X] # adds x0 = 1 to each instance  \ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)  \ntheta_best  \n  \n# array([[4.00634639], [2.94157425]])\n</code></pre>\n<p>We can see that we did not get 4 and 3, which should be the result, but it was close enough (we didn’t get it exactly because of gaussian noise).</p>\n<p>Let’s plot some prediction points as a line and understand how our model fit training data:</p>\n<p><img src=\"/images/different-regression-algorithms/4.webp\" alt=\"data predictions\"></p>\n<p>We can see that our line fits data points pretty well for a simple linear model. The computational complexity for this is heavy (typically O(n²) to O(n³) for inverting a matrix). The predictions are fast O(n).</p>\n<hr>\n<h2>Polynomial Regression</h2>\n<p>We can see that our data points were really simple in the previous model. But what if our data does not fit a simple straight line? Well, we can use a linear model to fit non-linear data. A simple way to do this is adding powers of each feature as new features, and train a linear model in this extended set of features. This technique is called Polynomial Regression.</p>\n<p>We can do this using PolynomialFeatures from scikit learn library. For example:</p>\n<pre><code class=\"language-python\">from sklearn.preprocessing import PolynomialFeatures  \n  \nm = 100  \nX = 6 * np.random.rand(m, 1) - 3  \ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)  \n  \npoly_features = PolynomialFeatures(degree=2, include_bias=false)  \nX_poly = poly_features.fit_transform(X)  \nX_poly[0]  \n  \n# array([-0.752734872, 0.5666234])\n</code></pre>\n<p>Using PolynomialFeatures allow us to add the square of each feature to the training model (in this case we have just one feature). Now, X_poly has the original feature X plus X squared. Note that when we have many features, Polynomial Regression is capable of finding relationships between them (something that a Linear Regression cannot do). This is possible because PolynomialFeatures also adds all combinations of features to the degree provided. This means if we had <em>a</em> and <em>b</em> with degree=3, we would have:</p>\n<p><em>a², a³, b², b³, ab, a²b, ab²</em></p>\n<hr>\n<h2>Ridge Regression</h2>\n<p>Ridge regression is a regularized version of linear regression. In general, for linear models, regularization is done by restricting model weights. In Ridge, a regularization term is added to the cost function. The hyperparameter used controls how much you want to regularize the model. Closer to 1, weights will end up close to 0 and the line close to the average. The polarization term is not regularized. An easy way to execute a ridge regression is:</p>\n<pre><code class=\"language-python\">from sklearn.linear_model import Ridge  \n  \n# assuming you already have X and y   \n  \nridge_reg = Ridge(alpha=1, solver=&quot;cholesky&quot;)  \nridge_reg.fit(X, y)  \nridge_reg.predict([[1.5]]) # predicting some value\n</code></pre>\n<p><img src=\"/images/different-regression-algorithms/5.webp\" alt=\"ridge regression equation\"></p>\n<hr>\n<h2>Lasso Regression</h2>\n<p>It is also a regularized version of linear regression, and it adds a regularization term to the cost functions, but it uses the norm <em>l1</em> to the weight instead of the half of the square of the norm <em>l2</em>. Lasso regression tends to eliminate completely the weights of features with less importance (adjust them to 0). It automatically executes feature selection and displays a sparse model (few feature weights != 0).</p>\n<pre><code class=\"language-py\">from sklearn.linear_model import Lasso  \nlasso_reg = Lasso(alpha=0.1)  \nlasso_reg.fit(X, y)  \nlasso_reg.predict([[1.5]])  \n  \n# array([8.28714555])\n</code></pre>\n<p><img src=\"/images/different-regression-algorithms/6.webp\" alt=\"lasso regression equation\"></p>\n<hr>\n<h2>Elastic Net</h2>\n<p>It is a mid term between Ridge and Lasso. The regularization term is simply a mix of the regularization terms of Ridge and Lasso, and you can control the ‘mixing rate’ r. When r=0, Elastic Net is the same thing as Ridge, and r=1 it is equal to Lasso.</p>\n<pre><code class=\"language-py\">from sklearn.linear_model import ElasticNet  \n  \n# l1 ratio l1_ratio is mixing tax  \nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  \nelastic_net.fit(X,y)  \nelastic_net.predict([[1.5]])  \n  \n# array([8.18453219])\n</code></pre>\n<p><img src=\"/images/different-regression-algorithms/7.webp\" alt=\"elastic net equation\"></p>\n<p>To choose between regression, we need to take some things in account. Generally speaking, it is not good to use linear regression without any regularization. Ridge is a good pattern, but if you can tell that some features are useless, you can choose between Lasso and Elastic Net, since they tend to reduce useless features to 0. In general, Elastic Net is preferable to Lasso, that can behave wrongly when feature number is higher than training instances number or highly correlated features.</p>\n<hr>\n<h2>Logistic Regression</h2>\n<p>Commonly used to estimate the probability of an instance belong to a determined category. It calculates the ponderated sum of input features and it generates the logistic of this result.</p>\n<p><img src=\"/images/different-regression-algorithms/8.webp\" alt=\"logistic regression equation\"></p>\n<p>The cost relation with all training set is simply the average cost related to all training instances, and it is written in an expression called log loss:</p>\n<p><img src=\"/images/different-regression-algorithms/9.webp\" alt=\"logistic loss\"></p>\n<hr>\n<h2>Softmax Regression</h2>\n<p>Logistic regression can be generalized for multiple classes directly without the need to train and combine many binary classifiers. This is called Softmax Regression, or Multinomial Logistic Regression.</p>\n<p>Given an instance x, softmax model will calculated a punctuation sk(x) to each class k, and then estimate the probability for each class applying the softmax function (normalized exponential) to the punctuations. Once calculated, you can estimate the probability of Phat k of an instance to belong to the class k. It calculates the exponencial of each punctuation and normalizes it by dividing it by the sum of all exponentials.</p>\n<hr>\n<h2>Conclusion</h2>\n<p>Regression algorithms are used very often to study and to predict linear (and even non linear) data. Linear regression is the most common, but it is important to know that different cases require different algorithms. That said, it is important to know how other algorithms work, so you can pick the right one when needed! Thanks for reading it, see you next week!</p>\n<p>— Lorenzo</p>\n<h2>References:</h2>\n<p>Mãos à Obra: Aprendizado de Máquina com Scikit-Learn &amp; TensorFlow — Aurélien Geron</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Linear_regression?source=post_page-----19f4442cc233--------------------------------\">Linear regression - Wikipedia</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Logistic_regression?source=post_page-----19f4442cc233--------------------------------\">Logistic regression - Wikipedia</a></p>\n<p><a href=\"https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3\">https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3</a></p>\n","title":"Understanding Different types of Regression","description":"A guide to understanding different types of regression algorithms","date":"2024-05-20"}},"__N_SSG":true}